{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448f6aa6",
   "metadata": {},
   "source": [
    "# Checking correlation of aspect scores with acceptance outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805a0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get these scores for all papers\n",
    "#dont necessarily need to store all the scores to get this but I may as well \n",
    "#set up a (nested?) dictionary in case there's other numerical analysis to be done\n",
    "\n",
    "#dictionary format { paper1: {name= id, originality = 1, ..., outcome = accept}}\n",
    "\n",
    "#y = acceptance (where true = 1, false = 0)?\n",
    "#x = aspect score\n",
    "#so logistic regression?\n",
    "\n",
    "#go through files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82645ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(paper):\n",
    "  metadata = paper.get('metadata') #metadata dictionary that contains the actual contents of the paper\n",
    "  prompt_not_structured_output = f\"\"\"\n",
    "\n",
    "  Please read the paper information below and predict whether this paper would be accepted or rejected at ICLR 2017. Then, explain your reasoning.\n",
    "  Respond only in the following JSON format:\n",
    "  {{\n",
    "    \"decision\": \"ACCEPTED\" or \"REJECTED\",\n",
    "    \"rationale\": \"Explain your reasoning\"\n",
    "  }}\n",
    "\n",
    "  Abstract: {metadata.get('abstractText', '').strip()}\n",
    "\n",
    "  \"\"\"\n",
    "  prompt1 = f\"\"\"\n",
    "\n",
    "  You will review the title and abstract of a research paper. In the JSON prediction field, provide your prediction of ACCEPT or REJECT for the paper's submission to ICLR 2017. \n",
    "  Then, in the JSON reasoning field, provide your reasoning for your prediction.\n",
    "\n",
    "  Abstract: {metadata.get('abstractText', '').strip()}\n",
    "\n",
    "  \"\"\"\n",
    "  #print(metadata)\n",
    "  #metadata.get('title', '').strip()\n",
    "  #metadata.get('abstractText', '').strip()\n",
    "  \n",
    "\n",
    "  prompt = f\"\"\"\n",
    "\n",
    "  You will review the contents of a research paper. \n",
    "\n",
    "  Paper Contents: {str(metadata.get('sections'))}\n",
    "\n",
    "  In the JSON prediction field, provide your prediction of ACCEPT or REJECT for the paper's submission to ICLR 2017. \n",
    "  Then, in the JSON reasoning field, provide your reasoning for your prediction.\n",
    "  \"\"\"\n",
    "  return prompt\n",
    "\n",
    "\n",
    "'''\n",
    "{\n",
    "  \"decision\": \"REJECTED\",\n",
    "  \"rationale\": \"The paper lacks novelty and the results are not clearly explained.\",\n",
    "  \"aspect_scores\": {\n",
    "    \"originality\": 2,\n",
    "    \"clarity\": 3,\n",
    "    \"soundness\": 2,\n",
    "    \"impact\": 2\n",
    "  }\n",
    "}\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecasting(model, prompt):\n",
    "    #print(prompt)\n",
    "    # Send request to Ollama\n",
    "\n",
    "\n",
    "    res = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model, #llama3.2:3b , \"qwen3:latest\"\n",
    "            \"prompt\": prompt, \n",
    "            \"stream\": False, \n",
    "            #\"think\": True,\n",
    "            # should i include format field?\n",
    "            \"format\":{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\":{ \"prediction\": {\"type\": \"string\"}, \"rationale\": {\"type\":\"string\"} }, \n",
    "            \"required\": [\"prediction\", \"reasoning\"]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = res.json()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01a8c",
   "metadata": {},
   "source": [
    "implementing structured json outputs - https://ollama.com/blog/structured-outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = [\"C:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\PeerRead\\\\data\\\\iclr_2017\\\\train\\\\parsed_pdfs\\\\304.pdf.json\"]\n",
    "review = [\"C:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\PeerRead\\\\data\\\\iclr_2017\\\\train\\\\reviews\\\\304.json\"]\n",
    "def get_response_prefinetuning(pdf_path, review_path, results):\n",
    "    \n",
    "    with open(pdf_path, 'r') as f1:\n",
    "        paper = json.load(f1) #json file contents for one research paper\n",
    "    with open(review_path, 'r') as f2:\n",
    "        review = json.load(f2)\n",
    "\n",
    "    prompt = build_prompt(paper)\n",
    "    model = \"qwen3:latest\"\n",
    "    output = model_forecasting(model, prompt)\n",
    "    json_response = json.loads(output.get(\"response\"))\n",
    "    \n",
    "    results[paper.get(\"name\")] = {\n",
    "        \"real_acceptance_label\": review.get(\"accepted\"),\n",
    "        \"predicted\": json_response.get(\"prediction\"),\n",
    "        \"rationale\": json_response.get(\"rationale\"),\n",
    "        \"complete_output\": output\n",
    "    }\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ceb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "iclr_parsed_train_path = \"C:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\PeerRead\\\\data\\\\iclr_2017\\\\train\\\\parsed_pdfs\"\n",
    "iclr_reviews_train_path = \"C:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\PeerRead\\\\data\\\\iclr_2017\\\\train\\\\reviews\"\n",
    "output_path = \"C:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\PeerRead\\\\dtais_summer\\\\qwen3_forecasting_paper_100.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_accuracy(parsed_path, reviews_path):\n",
    "    sorted_paper_paths= sorted(os.listdir(parsed_path))\n",
    "    sorted_review_paths = sorted(os.listdir(reviews_path))\n",
    "    results = {} #dictionary of results\n",
    "\n",
    "    for paper_json_file, review_json_file in zip(sorted_paper_paths[:100], sorted_review_paths[:100]):\n",
    "        json_pdf_path = os.path.join(parsed_path, paper_json_file)\n",
    "        json_review_path = os.path.join(reviews_path, review_json_file)\n",
    "        #print(paper_json_file)\n",
    "        results = get_response_prefinetuning(json_pdf_path, json_review_path, results)\n",
    "        \n",
    "    with open(output_path,'a') as f3:\n",
    "        json.dump(results,f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(iclr_parsed_train_path, iclr_reviews_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_results_path ='C:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\PeerRead\\\\dtais_summer\\\\qwen3_forecasting_paper_100.json'\n",
    "with open(json_results_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#Extract true and predicted labels \n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for fname, entry in data.items():\n",
    "    #print(data.items())\n",
    "    true_label = entry.get(\"real_acceptance_label\", None)\n",
    "    pred_label = entry.get(\"predicted\").strip().upper()\n",
    "\n",
    "    if true_label is None or pred_label not in [\"ACCEPT\", \"REJECT\"]:\n",
    "        print(f'missing label for {fname}')\n",
    "        continue  #in case entry is not in epxected format\n",
    "\n",
    "    true_label_str = \"ACCEPT\" if true_label else \"REJECT\"\n",
    "    y_pred.append(pred_label)\n",
    "\n",
    "if y_true:\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {acc:.2f} on {len(y_true)} examples\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "else:\n",
    "    print(\"No valid data to evaluate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to set a random seed to sample for evaluating accuracy or just shuffle and prompt for all ICLR training papers to get accuracy score?\n",
    "#should keep track of what files contain true vs false for accepted and how uneven this distribution is!\n",
    "\n",
    "accepted = []\n",
    "rejected = []\n",
    "\n",
    "def sort_by_acceptance(paper_path, review_path):\n",
    "    for paper_review_pair in zip(os.listdir(paper_path), os.listdir(review_path)):\n",
    "        fpaper_path = os.path.join(paper_path, paper_review_pair[0])\n",
    "        freview_path = os.path.join(review_path, paper_review_pair[1])\n",
    "        try: \n",
    "            with open(freview_path) as f:\n",
    "                review_data = json.load(f)\n",
    "\n",
    "            outcome = review_data.get(\"accepted\")\n",
    "            #file_basename = os.path.basename(freview_path)\n",
    "            #https://stackoverflow.com/questions/678236/how-do-i-get-the-filename-without-the-extension-from-a-path-in-python\n",
    "            \n",
    "            #paper_id = os.path.splitext(file_basename)[0]\n",
    "            if outcome == True: accepted.append([fpaper_path, freview_path])\n",
    "            elif outcome == False: rejected.append([fpaper_path, freview_path])\n",
    "            else: print(f\"CHECK FILE {freview_path}: accepted field contains {outcome}\")\n",
    "        except Exception as e: \n",
    "            print(f\"error reading {freview_path}: {e}\")\n",
    "            \n",
    "    return sorted(accepted), sorted(rejected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted, rejected = sort_by_acceptance(iclr_parsed_train_path, iclr_reviews_train_path)\n",
    "#print(f\"accepted: {accepted}\")\n",
    "#print(f\"rejected: {rejected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_sample(random_sampling, half_num_samples, set_seed, parsed_path, reviews_path, output_path):\n",
    "    random.seed(set_seed)\n",
    "    paper_name_set = []\n",
    "    results = {}\n",
    "    if random_sampling: #set random_sampling = True to randomly sample a subset of papers in the folder. \n",
    "        all_accepted, all_rejected = sort_by_acceptance(parsed_path, reviews_path)\n",
    "        accepted = random.sample(all_accepted, half_num_samples)\n",
    "        rejected = random.sample(all_rejected, half_num_samples)\n",
    "        paper_name_set = paper_name_set + accepted + rejected\n",
    "        #number of accepted papers = number of rejected papers in sample set\n",
    "        random.shuffle(paper_name_set)\n",
    "\n",
    "    #print(accepted)\n",
    "    #print(rejected)\n",
    "    #print(paper_name_set)\n",
    "    \n",
    "    if random_sampling==False:\n",
    "        paper_name_set =  zip(os.list(parsed_path), os.list(reviews_path)) #note: train and review folders contain the same \n",
    "        random.shuffle(paper_name_set)\n",
    "        for i in range(0, len(paper_name_set)):\n",
    "            paper_name_set[i] = [os.path.join(parsed_path, i)[0], os.path.join(reviews_path, i)[1]]\n",
    "\n",
    "    #print(paper_name_set[0])\n",
    "    for json_pdf_path, json_review_path in paper_name_set:\n",
    "        results = get_response_prefinetuning(json_pdf_path, json_review_path, results)\n",
    "    with open(output_path,'a') as f3:\n",
    "        json.dump(results,f3)\n",
    "    \n",
    "    return paper_name_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"C:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\PeerRead\\\\dtais_summer\\\\qwen3_forecasting_paper_100.json\"\n",
    "prompt_sample(random_sampling = True, half_num_samples = 50, set_seed = 50, parsed_path = iclr_parsed_train_path, reviews_path = iclr_reviews_train_path, output_path = output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtais_demo_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
