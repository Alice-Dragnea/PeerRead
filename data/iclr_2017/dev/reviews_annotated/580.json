{"conference": "ICLR 2017 conference submission", "title": "Intelligible Language Modeling with Input Switched Affine Networks", "abstract": "The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question.  There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters.  It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences.  As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "Summary: The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve", "RECOMMENDATION": 4}, {"CLARITY": 5, "is_meta_review": false, "comments": "The authors present a character language model that gains some interpretability without large losses in predictivity. CONTRIBUTION: I'd characterize the paper as some experimental investigation of a c", "IS_META_REVIEW": false}, {"SUBSTANCE": 5, "comments": "Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias bas", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 5, "is_meta_review": false, "RECOMMENDATION": 5, "CLARITY": 5, "IS_META_REVIEW": false}], "authors": "Jakob Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-dickstein, David Sussillo", "accepted": false, "id": "580"}