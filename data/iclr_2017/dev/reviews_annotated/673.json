{"conference": "ICLR 2017 conference submission", "title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.", "histories": [], "reviews": [{"IMPACT": 2, "MEANINGFUL_COMPARISON": 3, "comments": "I find this paper not very compelling. The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable. However, this was precisely ", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "is_meta_review": false, "CLARITY": 4, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer. 2. It shows the 10-mips > 100-mips > 1000-mips, does it me", "SOUNDNESS_CORRECTNESS": 2}, {"IS_META_REVIEW": false, "SOUNDNESS_CORRECTNESS": 3, "is_meta_review": false, "comments": "The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is no", "MEANINGFUL_COMPARISON": 3}], "authors": "Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio", "accepted": false, "id": "673"}