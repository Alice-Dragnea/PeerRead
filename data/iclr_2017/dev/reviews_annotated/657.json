{"conference": "ICLR 2017 conference submission", "title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "histories": [], "reviews": [{"IMPACT": 4, "comments": "The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "is_meta_review": false, "CLARITY": 4, "IS_META_REVIEW": false}, {"MEANINGFUL_COMPARISON": 4, "comments": "The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via pro", "SOUNDNESS_CORRECTNESS": 5, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}, {"comments": "This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastT", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 1, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}], "authors": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, Tomas Mikolov", "accepted": false, "id": "657"}