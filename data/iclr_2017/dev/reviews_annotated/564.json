{"conference": "ICLR 2017 conference submission", "title": "Higher Order Recurrent Neural Networks", "abstract": "In this paper, we study novel neural network structures to better model long term dependency in sequential data.  We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.", "histories": [], "reviews": [{"IMPACT": 3, "comments": "This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) ste", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "is_meta_review": false, "CLARITY": 2, "IS_META_REVIEW": false}, {"SUBSTANCE": 2, "comments": "The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that inform", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "is_meta_review": false, "RECOMMENDATION": 2, "IS_META_REVIEW": false}, {"IMPACT": 4, "SUBSTANCE": 3, "comments": "I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from ", "is_meta_review": false, "CLARITY": 4, "IS_META_REVIEW": false}], "authors": "Rohollah Soltani, Hui Jiang", "accepted": false, "id": "564"}