{"conference": "ICLR 2017 conference submission", "title": "On Detecting Adversarial Perturbations", "abstract": "Machine learning and  deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small ``detector'' subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust.  We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.", "histories": [], "reviews": [{"ORIGINALITY": 5, "is_meta_review": false, "comments": "I reviewed the manuscript on December 5th. Summary: The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial da", "IS_META_REVIEW": false}, {"ORIGINALITY": 5, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easi", "RECOMMENDATION": 5}, {"ORIGINALITY": 4, "CLARITY": 5, "is_meta_review": false, "comments": "This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks. This takes the competition between adversari", "IS_META_REVIEW": false}], "authors": "Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff", "accepted": true, "id": "462"}