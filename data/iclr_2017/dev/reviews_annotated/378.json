{"conference": "ICLR 2017 conference submission", "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards", "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring only small modifications to the standard REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. We find that our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Notably, the approach is able to solve a benchmark multi-digit addition task. To our knowledge, this is the first time that a pure RL method has solved addition using only reward feedback.", "histories": [], "reviews": [{"SUBSTANCE": 3, "comments": "The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current polic", "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"IMPACT": 4, "is_meta_review": false, "comments": "This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and ex", "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "comments": "overview: This work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and th", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 5, "is_meta_review": false, "RECOMMENDATION": 5, "CLARITY": 5, "IS_META_REVIEW": false}], "authors": "Ofir Nachum, Mohammad Norouzi, Dale Schuurmans", "accepted": true, "id": "378"}