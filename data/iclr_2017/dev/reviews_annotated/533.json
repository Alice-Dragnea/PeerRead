{"conference": "ICLR 2017 conference submission", "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning", "abstract": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivat", "MEANINGFUL_COMPARISON": 4}, {"ORIGINALITY": 4, "is_meta_review": false, "comments": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learn", "IS_META_REVIEW": false}, {"ORIGINALITY": 2, "is_meta_review": false, "comments": "This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in int", "IS_META_REVIEW": false}], "authors": "Joshua Achiam, Shankar Sastry", "accepted": false, "id": "533"}