{"conference": "ICLR 2017 conference submission", "title": "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?", "abstract": "The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gait cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.", "histories": [], "reviews": [{"CLARITY": 5, "is_meta_review": false, "comments": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment. My biggest point of critique is that it's difficult to draw conclusions or reason beyond th", "IS_META_REVIEW": false}, {"IMPACT": 3, "APPROPRIATENESS": 2, "comments": "Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD cont", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 2, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"IMPACT": 3, "CLARITY": 5, "is_meta_review": false, "comments": "The paper is straightforward, easy to read, and has clear results. Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is ", "IS_META_REVIEW": false}], "authors": "Xue Bin Peng, Michiel van de Panne", "accepted": false, "id": "775"}