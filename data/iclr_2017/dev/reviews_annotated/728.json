{"conference": "ICLR 2017 conference submission", "title": "Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding", "abstract": "Recent progress in the field of Reinforcement Learning (RL) has enabled to tackle bigger and more challenging tasks. However, the increasing complexity of the problems, as well as the use of more sophisticated models such as Deep Neural Networks (DNN), impedes the understanding of artificial agents behavior. In this work, we present the Semi-Aggregated Markov Decision Process (SAMDP) model. The purpose of the SAMDP modeling is to describe and allow a better understanding of complex behaviors by identifying temporal and spatial abstractions. In contrast to other modeling approaches, SAMDP is built in a transformed state-space that encodes the dynamics of the problem. We show that working with the \\emph{right} state representation mitigates the problem of finding spatial and temporal abstractions. We describe the process of building the SAMDP model from observed trajectories and give examples for using it in a toy problem and complicated DQN policies. Finally, we show how using the SAMDP we can monitor the policy at hand and make it more robust.", "histories": [], "reviews": [{"ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order t", "MEANINGFUL_COMPARISON": 4}, {"IMPACT": 2, "comments": "The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model cal", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false}, {"ORIGINALITY": 2, "CLARITY": 3, "is_meta_review": false, "comments": "The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a ", "IS_META_REVIEW": false}], "authors": "Nir Baram, Tom Zahavy, Shie Mannor", "accepted": false, "id": "728"}