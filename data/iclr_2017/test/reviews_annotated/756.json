{"conference": "ICLR 2017 conference submission", "title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data. Furthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 4, "comments": "this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context", "ORIGINALITY": 2, "is_meta_review": false, "IS_META_REVIEW": false}, {"comments": "This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple ex", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 3, "IS_META_REVIEW": false, "RECOMMENDATION": 2, "is_meta_review": false}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "comments": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. Although the paper presents this notion as new, basically every pre-train", "ORIGINALITY": 2, "IS_META_REVIEW": false, "is_meta_review": false}], "authors": "Franziska Horn, Klaus-Robert M\u00fcller", "accepted": false, "id": "756"}