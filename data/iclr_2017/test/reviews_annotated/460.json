{"conference": "ICLR 2017 conference submission", "title": "Sample Efficient Actor-Critic with  Experience Replay", "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.", "histories": [], "reviews": [{"CLARITY": 3, "is_meta_review": false, "comments": "This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method c", "IS_META_REVIEW": false}, {"IMPACT": 4, "CLARITY": 5, "is_meta_review": false, "comments": "The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain. The paper reads a bit like a laundry list of the researchers latest tricks. It is w", "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning ", "SOUNDNESS_CORRECTNESS": 4}], "authors": "Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando de Freitas", "accepted": true, "id": "460"}