{"conference": "ICLR 2017 conference submission", "title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 3, "comments": "The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behav", "ORIGINALITY": 2, "is_meta_review": false, "RECOMMENDATION": 3, "IS_META_REVIEW": false}, {"CLARITY": 3, "is_meta_review": false, "comments": "I appreciate the work but I do not think the paper is clear enough. Moreover, the authors say \"local minimia\" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessa", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficie", "IS_META_REVIEW": false}], "authors": "Daniel Jiwoong Im, Michael Tao, Kristin Branson", "accepted": false, "id": "556"}