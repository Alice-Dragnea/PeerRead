{"conference": "ICLR 2017 conference submission", "title": "An Actor-critic Algorithm for Learning Rate Learning", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. To avoid manually searching of learning rates, which is tedious and inefficient, we propose an algorithm to automatically learn learning rates using actor-critic methods from reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. Experiments show that our method leads to good convergence of SGD and can prevent overfitting to a certain extent, resulting in better performance than human-designed competitors.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 3, "comments": "In the question response the authors mention and compare other works such as \"Learning to Learn by Gradient Descent by Gradient Descent\", but the goal of current work and that work is quite different.", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "is_meta_review": false, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "SOUNDNESS_CORRECTNESS": 3, "is_meta_review": false, "comments": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in ", "MEANINGFUL_COMPARISON": 3}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented agai", "SOUNDNESS_CORRECTNESS": 4}], "authors": "Chang Xu, Tao Qin, Gang Wang, Tie-Yan Liu", "accepted": false, "id": "767"}