{"conference": "ICLR 2017 conference submission", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "This paper poses an interesting idea: removing chaotic behavior or RNNs. While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis pa", "SOUNDNESS_CORRECTNESS": 5}, {"ORIGINALITY": 4, "is_meta_review": false, "comments": "I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it ", "IS_META_REVIEW": false}, {"ORIGINALITY": 5, "is_meta_review": false, "comments": "The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architectu", "IS_META_REVIEW": false}], "authors": "Thomas Laurent, James von Brecht", "accepted": true, "id": "398"}