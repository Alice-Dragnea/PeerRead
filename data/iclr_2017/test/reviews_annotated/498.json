{"conference": "ICLR 2017 conference submission", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "histories": [], "reviews": [{"IMPACT": 3, "is_meta_review": false, "comments": "This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout inference gap which they define to be the gap between network output during trai", "IS_META_REVIEW": false}, {"IMPACT": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "summary The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum ", "SOUNDNESS_CORRECTNESS": 5}, {"IMPACT": 4, "comments": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantit", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false}], "authors": "Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng, Eduard Hovy", "accepted": true, "id": "498"}