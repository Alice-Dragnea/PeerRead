{"conference": "ICLR 2017 conference submission", "title": "Efficient Vector Representation for Documents through Corruption", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.", "histories": [], "reviews": [{"SUBSTANCE": 3, "is_meta_review": false, "comments": "This paper presents a framework for creating document representations. The main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors in", "IS_META_REVIEW": false}, {"ORIGINALITY": 2, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces", "RECOMMENDATION": 5}, {"ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with var", "RECOMMENDATION": 5}], "authors": "Minmin Chen", "accepted": true, "id": "330"}