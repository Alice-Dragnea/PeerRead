{"conference": "ICLR 2017 conference submission", "title": "Nonparametrically Learning Activation Functions in Deep Neural Nets", "abstract": "We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justification for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric activation functions generalize well. To demonstrate the power of our novel techniques, we test them on image recognition datasets and achieve up to a 15% relative increase in test performance compared to the baseline.", "histories": [], "reviews": [{"is_meta_review": false, "comments": "This paper describes an approach to learning the non-linear activation function in deep neural nets. This is achieved by representing the activation function in a basis of non-linear functions and lea", "IS_META_REVIEW": false}, {"ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "Summary: The paper introduces a parametric class for non linearities used in neural networks. The paper suggests two stage optimization to learn the weights of the network, and the non linearity weigh", "SOUNDNESS_CORRECTNESS": 5}, {"ORIGINALITY": 5, "CLARITY": 5, "is_meta_review": false, "comments": "This paper provides a principled framework for nonparametrically learning activation functions in deep neural networks. A theoretical justification for authors' choice of nonparametric activation func", "IS_META_REVIEW": false}], "authors": "Carson Eisenach, Zhaoran Wang, Han Liu", "accepted": false, "id": "512"}