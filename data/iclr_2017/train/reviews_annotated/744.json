{"conference": "ICLR 2017 conference submission", "title": "Learning Identity Mappings with Residual Gates", "abstract": "We propose a layer augmentation technique that adds shortcut connections with a linear gating mechanism, and can be applied to almost any network model. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Highway Neural Networks and Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. In our experiments, augmented plain networks -- which can be interpreted as simplified Highway Neural Networks -- outperform ResNets, raising new questions on how shortcut connections should be designed. We also evaluate our model on CIFAR-10 and CIFAR-100 using augmented Wide ResNets, achieving 3.65% and 18.27% test error, respectively.", "histories": [], "reviews": [{"ORIGINALITY": 2, "is_meta_review": false, "comments": "This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly uti", "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "is_meta_review": false, "comments": "The paper presents a layer architecture where a single parameter is used to gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of ", "IS_META_REVIEW": false}], "authors": "Pedro H. P. Savarese, Leonardo O. Mazza, Daniel R. Figueiredo", "accepted": false, "id": "744"}