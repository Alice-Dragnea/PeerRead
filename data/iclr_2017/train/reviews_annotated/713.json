{"conference": "ICLR 2017 conference submission", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes a modification of the ELU activation function for neural networks, by parameterizing it with 2 trainable parameters per layer. This parameter is proposed to more effectively counte", "SOUNDNESS_CORRECTNESS": 3}, {"SUBSTANCE": 2, "is_meta_review": false, "comments": "The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function. Proposed is an approach of learning the activation functions during the training pro", "IS_META_REVIEW": false}, {"is_meta_review": false, "comments": "Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present b", "IS_META_REVIEW": false}, {"IMPACT": 4, "is_meta_review": false, "comments": "This paper presents a new non-linear function for CNN and deep neural networks. The new non-linearity reports some gains on most datasets of interest, and can be used in production networks with minim", "IS_META_REVIEW": false}], "authors": "Ludovic Trottier, Philippe Gigu\u00e8re, Brahim Chaib-draa", "accepted": false, "id": "713"}