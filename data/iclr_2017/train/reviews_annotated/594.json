{"conference": "ICLR 2017 conference submission", "title": "Low-rank passthrough neural networks", "abstract": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. For large depths, this is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem, e.g. LSTMs, GRUs, Highway Networks and Deep Residual Networks, which are based on a single structural principle: the state passthrough. We observe that these \"Passthrough Networks\" architectures enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on several tasks, including a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.", "histories": [], "reviews": [{"SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 2, "comments": "The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network. Author also presents as a contribution the pas", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "is_meta_review": false, "IS_META_REVIEW": false}, {"IMPACT": 2, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments. That said, I found the results not very convi", "SOUNDNESS_CORRECTNESS": 3}, {"SUBSTANCE": 3, "is_meta_review": false, "comments": "The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal", "IS_META_REVIEW": false}], "authors": "Antonio Valerio Miceli Barone", "accepted": false, "id": "594"}