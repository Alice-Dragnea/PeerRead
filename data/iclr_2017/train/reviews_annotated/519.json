{"conference": "ICLR 2017 conference submission", "title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.  We evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "histories": [], "reviews": [{"SUBSTANCE": 2, "CLARITY": 3, "is_meta_review": false, "comments": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The methods gradient behaviour is analysed. Experimental results seem to indicate that the", "IS_META_REVIEW": false}, {"comments": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works be", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_META_REVIEW": false, "APPROPRIATENESS": 2, "is_meta_review": false}, {"IMPACT": 5, "comments": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well. The contribution is potentially useful to a ", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "IS_META_REVIEW": false, "is_meta_review": false}], "authors": "C\u00e9sar Laurent, Nicolas Ballas, Pascal Vincent", "accepted": false, "id": "519"}