{"conference": "ICLR 2017 conference submission", "title": "Frustratingly Short Attention Spans in Neural Language Modeling", "abstract": "Current language modeling architectures often use recurrent neural networks. Recently, various methods for incorporating differentiable memory into these architectures have been proposed. When predicting the next token, these models query information from a memory of the recent history and thus can facilitate learning mid- and long-range dependencies. However, conventional attention models produce a single output vector per time step that is used for predicting the next token as well as the key and value of a differentiable memory of the history of tokens. In this paper, we propose a key-value attention mechanism that produces separate representations for the key and value of a memory, and for a representation that encodes the next-word distribution. This usage of past memories outperforms existing memory-augmented neural language models on two corpora. Yet, we found that it mainly utilizes past memory only of the previous five representations. This led to the unexpected main finding that a much simpler model which simply uses a concatenation of output representations from the previous three-time steps is on par with more sophisticated memory-augmented neural language models.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the ", "SOUNDNESS_CORRECTNESS": 5}, {"ORIGINALITY": 4, "is_meta_review": false, "comments": "This paper focusses on attention for neural language modeling and has two major contributions: 1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a s", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has acces", "IS_META_REVIEW": false}], "authors": "Micha\u0142 Daniluk, Tim Rockt\u00e4schel, Johannes Welbl, Sebastian Riedel", "accepted": true, "id": "420"}