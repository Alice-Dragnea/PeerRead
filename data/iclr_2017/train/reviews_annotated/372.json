{"conference": "ICLR 2017 conference submission", "title": "Learning to Remember Rare Events", "abstract": "Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision.  It operates in a life-long manner, i.e., without the need to reset it during training.  Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task.", "histories": [], "reviews": [{"IMPACT": 5, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show", "SOUNDNESS_CORRECTNESS": 5}, {"MEANINGFUL_COMPARISON": 1, "comments": "The paper proposes a new memory module to be used as an addition to existing neural network models. Pros: * Clearly written and original idea. * Useful memory module, shows nice improvements. * Tested", "ORIGINALITY": 5, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}, {"IMPACT": 4, "comments": "A new memory module based on k-NN is presented. The paper is very well written and the results are convincing. Omniglot is a good sanity test and the performance is surprisingly good. The artificial t", "SOUNDNESS_CORRECTNESS": 5, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}], "authors": "Lukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio", "accepted": true, "id": "372"}