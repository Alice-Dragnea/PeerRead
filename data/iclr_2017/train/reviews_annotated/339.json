{"conference": "ICLR 2017 conference submission", "title": "Improving Neural Language Models with a Continuous Cache", "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.", "histories": [], "reviews": [{"ORIGINALITY": 2, "is_meta_review": false, "comments": "This paper proposes a simple extension to a neural network language model by adding a cache component. The model stores", "IS_META_REVIEW": false}, {"comments": "The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with ", "ORIGINALITY": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 4, "APPROPRIATENESS": 2, "is_meta_review": false}, {"comments": "This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture thi", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 3, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "is_meta_review": false}], "authors": "Edouard Grave, Armand Joulin, Nicolas Usunier", "accepted": true, "id": "339"}