{"conference": "ICLR 2017 conference submission", "title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.", "histories": [], "reviews": [{"IMPACT": 5, "SUBSTANCE": 3, "comments": "The paper claim that, when supported by a number of backup workers, synchronized-SGD actually works better than async-SGD. The paper first analyze the problem of staled updates in async-SGDs, and prop", "ORIGINALITY": 4, "is_meta_review": false, "IS_META_REVIEW": false}, {"IMPACT": 4, "SUBSTANCE": 3, "comments": "This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, whi", "ORIGINALITY": 3, "is_meta_review": false, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "This paper was easy to read, the main idea was presented very clearly. The main points of the paper (and my concerns are below) can be summarized as follows: 1. synchronous algoriths suffer from some ", "SOUNDNESS_CORRECTNESS": 3}], "authors": "Jianmin Chen*, Xinghao Pan*, Rajat Monga, Samy Bengio, Rafal Jozefowicz", "accepted": false, "id": "783"}