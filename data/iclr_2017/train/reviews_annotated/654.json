{"conference": "ICLR 2017 conference submission", "title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "histories": [], "reviews": [{"ORIGINALITY": 3, "SUBSTANCE": 3, "is_meta_review": false, "comments": "This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. O", "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "comments": "The authors present a method for training probabilistic models by maximizing a stochastic variational-lower-bound-type objective. Training involves sampling and then learning a transition-based infere", "ORIGINALITY": 4, "IS_META_REVIEW": false, "RECOMMENDATION": 2, "is_meta_review": false}, {"ORIGINALITY": 3, "CLARITY": 3, "is_meta_review": false, "comments": "I very much like the underlying idea for this paper. I wasn't convinced by the execution in its current state. My primary concern is the one I expressed in my pre-review question below, which I don't ", "IS_META_REVIEW": false}], "authors": "Anirudh Goyal, Nan Rosemary Ke, Alex Lamb, Yoshua Bengio", "accepted": false, "id": "654"}