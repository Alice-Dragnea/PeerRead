{"conference": "ICLR 2017 conference submission", "title": "Efficient Summarization with Read-Again and Copy Mechanism", "abstract": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.", "histories": [], "reviews": [{"CLARITY": 4, "is_meta_review": false, "comments": "Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice a", "IS_META_REVIEW": false}, {"CLARITY": 3, "is_meta_review": false, "comments": "This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the sour", "IS_META_REVIEW": false}, {"ORIGINALITY": 3, "is_meta_review": false, "comments": "This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms. 1. This paper introduces 2-pass ", "IS_META_REVIEW": false}], "authors": "Wenyuan Zeng, Wenjie Luo, Sanja Fidler, Raquel Urtasun", "accepted": false, "id": "630"}