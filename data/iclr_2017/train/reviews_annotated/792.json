{"conference": "ICLR 2017 conference submission", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "histories": [], "reviews": [{"SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 4, "comments": "Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of ", "SOUNDNESS_CORRECTNESS": 3, "IS_META_REVIEW": false, "is_meta_review": false}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 3, "comments": "This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Ben", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "is_meta_review": false, "IS_META_REVIEW": false}, {"SUBSTANCE": 2, "comments": "The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6", "ORIGINALITY": 3, "is_meta_review": false, "RECOMMENDATION": 2, "CLARITY": 4, "IS_META_REVIEW": false}], "authors": "Armen Aghajanyan", "accepted": false, "id": "792"}