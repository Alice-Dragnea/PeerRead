{"conference": "ICLR 2017 conference submission", "title": "Deep Variational Information Bottleneck", "abstract": "We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \u201cDeep Variational Information Bottleneck\u201d, or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.", "histories": [], "reviews": [{"CLARITY": 5, "is_meta_review": false, "comments": "Summary: The paper Deep Variational Information Bottleneck explores the optimization of neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999). On the e", "IS_META_REVIEW": false}, {"comments": "Thank you for an interesting read. I personally like the information bottleneck principle and am very happy to see its application to deep neural networks. To my knowledge, this is the first paper tha", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "is_meta_review": false, "RECOMMENDATION": 2, "CLARITY": 5, "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "Update: raised the score, because I think the arguments about adversarial examples are compelling. I think that the paper convincingly proves that this method acts as a decent regularizer, but I'm not", "IS_META_REVIEW": false}], "authors": "Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy", "accepted": true, "id": "442"}