{"conference": "ICLR 2017 conference submission", "title": "Multiplicative LSTM for sequence modelling", "abstract": "We introduce multiplicative LSTM (mLSTM), a novel recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. mLSTM is characterised by its ability to have different recurrent transition functions for each possible input, which we argue makes it more expressive for autoregressive density estimation. We demonstrate empirically that mLSTM outperforms standard LSTM and its deep variants for a range of character level modelling tasks, and that this improvement increases with the complexity of the task. This model achieves a test error of 1.19 bits/character on the last 4 million characters of the Hutter prize dataset when combined with dynamic evaluation.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 1, "MEANINGFUL_COMPARISON": 3, "comments": "* Brief Summary: This paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level", "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "comments": "This paper proposes an extension of the multiplicative RNN [1] where the authors apply the same reparametrization trick to the weight matrices of the LSTM. The paper proposes some interesting tricks, ", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "Pros: * Clearly written. * New model mLSTM which seems to be useful according to the results. * Some interesting experiments on big data. Cons: * Number of parameters in comparisons of different model", "IS_META_REVIEW": false}], "authors": "Ben Krause, Iain Murray, Steve Renals, Liang Lu", "accepted": false, "id": "527"}