{"conference": "ICLR 2017 conference submission", "title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.", "histories": [], "reviews": [{"is_meta_review": false, "comments": "Overall, this is a nice paper. Developing a unifying framework for these newer neural models is a worthwhile endeavor. However, it's unclear if the DRAGNN framework (in its current form) is a signific", "IS_META_REVIEW": false}, {"is_meta_review": false, "comments": "The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a tra", "IS_META_REVIEW": false}, {"APPROPRIATENESS": 5, "comments": "The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamica", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "CLARITY": 4, "is_meta_review": false}], "authors": "Lingpeng Kong, Chris Alberti, Daniel Andor, Ivan Bogatyy, David Weiss", "accepted": false, "id": "671"}