{"conference": "ICLR 2017 conference submission", "title": "Neural Data Filter for Bootstrapping Stochastic Gradient Descent", "abstract": "Mini-batch based Stochastic Gradient Descent(SGD) has been widely used to train deep neural networks efficiently. In this paper, we design a general framework to automatically and adaptively select training data for SGD. The framework is based on neural networks and we call it \\emph{\\textbf{N}eural \\textbf{D}ata \\textbf{F}ilter} (\\textbf{NDF}). In Neural Data Filter, the whole training process of the original neural network is monitored and supervised by a deep reinforcement network, which controls whether to filter some data in sequentially arrived mini-batches so as to maximize future accumulative reward (e.g., validation accuracy). The SGD process accompanied with NDF is able to use less data and converge faster while achieving comparable accuracy as the standard SGD trained on the full dataset. Our experiments show that NDF bootstraps SGD training for different neural network models including Multi Layer Perceptron Network and Recurrent Neural Network trained on various types of tasks including image classification and text understanding.", "histories": [], "reviews": [{"SUBSTANCE": 3, "comments": "Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one. But I still think there is lack of experim", "SOUNDNESS_CORRECTNESS": 4, "is_meta_review": false, "RECOMMENDATION": 2, "CLARITY": 5, "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "comments": "This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn opti", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "Paper is easy to follow, Idea is pretty clear and makes sense. Experimental results are hard to judge, it would be nice to have other baselines. For faster training convergence, the question is how we", "IS_META_REVIEW": false}], "authors": "Yang Fan, Fei Tian, Tao Qin, Tie-Yan Liu", "accepted": false, "id": "532"}