{"conference": "ICLR 2017 conference submission", "title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention  beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "histories": [], "reviews": [{"comments": "This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental aten", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "is_meta_review": false, "RECOMMENDATION": 5, "CLARITY": 5, "IS_META_REVIEW": false}, {"IMPACT": 4, "CLARITY": 5, "is_meta_review": false, "comments": "This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the inp", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "The authors propose to extend the standard attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variable", "IS_META_REVIEW": false}], "authors": "Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush", "accepted": true, "id": "393"}