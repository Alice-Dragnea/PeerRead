{"conference": "ICLR 2017 conference submission", "title": "Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear", "abstract": "To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically relive catastrophic mistakes. Many real-world environments where people might be injured exhibit a special structure. We know a priori that catastrophes are not only bad, but that agents need not ever get near to a catastrophe state. In this paper, we exploit this structure to learn a reward-shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective, shaping the reward function away from catastrophic states.", "histories": [], "reviews": [{"IMPACT": 5, "MEANINGFUL_COMPARISON": 3, "comments": "This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea). 1) insufficient context of what is known and had been studied before (in sh", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}, {"is_meta_review": false, "comments": "- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compa", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\". The paper is well written including some rather", "IS_META_REVIEW": false}], "authors": "Zachary C. Lipton, Jianfeng Gao, Lihong Li, Jianshu Chen, Li Deng", "accepted": false, "id": "751"}