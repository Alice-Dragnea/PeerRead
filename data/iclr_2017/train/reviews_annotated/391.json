{"conference": "ICLR 2017 conference submission", "title": "Exploring Sparsity in Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8\u00d7 and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse GEMMs. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2\u00d7 to 7\u00d7.", "histories": [], "reviews": [{"CLARITY": 5, "is_meta_review": false, "comments": "Updated review: 18 Jan. 2017 Thanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012. The comparison is plausible, though it would be clearer if", "IS_META_REVIEW": false}, {"MEANINGFUL_COMPARISON": 1, "comments": "The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech", "ORIGINALITY": 2, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}, {"is_meta_review": false, "comments": "Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model", "IS_META_REVIEW": false}], "authors": "Sharan Narang, Greg Diamos, Shubho Sengupta, Erich Elsen", "accepted": true, "id": "391"}