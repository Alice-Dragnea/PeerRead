{"conference": "ICLR 2017 conference submission", "title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "histories": [], "reviews": [{"CLARITY": 4, "is_meta_review": false, "comments": "*** Paper Summary *** The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer.", "IS_META_REVIEW": false}, {"ORIGINALITY": 5, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total n", "SOUNDNESS_CORRECTNESS": 5}, {"IS_META_REVIEW": false, "CLARITY": 3, "is_meta_review": false, "comments": "Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way. Unfortunately, as the results show, the authors could", "RECOMMENDATION": 3}], "authors": "David Ha, Andrew M. Dai, Quoc V. Le", "accepted": true, "id": "499"}