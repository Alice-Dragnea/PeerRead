{"conference": "ICLR 2017 conference submission", "title": "Learning Graphical State Transitions", "abstract": "Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that produces sequences from graph-structured input. In this work I introduce the Gated Graph Transformer Neural Network (GGT-NN), an extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines.", "histories": [], "reviews": [{"is_meta_review": false, "comments": "The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. T", "IS_META_REVIEW": false}, {"comments": "This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, thoug", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "is_meta_review": false, "RECOMMENDATION": 5, "CLARITY": 3, "IS_META_REVIEW": false}, {"ORIGINALITY": 4, "APPROPRIATENESS": 3, "is_meta_review": false, "comments": "The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method t", "IS_META_REVIEW": false}], "authors": "Daniel D. Johnson", "accepted": true, "id": "318"}