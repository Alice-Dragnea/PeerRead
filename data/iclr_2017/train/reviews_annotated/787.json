{"conference": "ICLR 2017 conference submission", "title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper presents the semantic embedding model for multi-label prediction. In my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said", "SOUNDNESS_CORRECTNESS": 3}, {"SUBSTANCE": 2, "comments": "The paper proposes a semantic embedding based approach to multilabel classification. Conversely to previous proposals, SEM considers the underlying parameters determining the observed labels are low-r", "ORIGINALITY": 2, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"comments": "This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itse", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 3, "is_meta_review": false}], "authors": "Liping Jing, MiaoMiao Cheng, Liu Yang, Alex Gittens, Michael W. Mahoney", "accepted": false, "id": "787"}