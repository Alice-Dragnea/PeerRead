{"conference": "ICLR 2017 conference submission", "title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications. However, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly. Even worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model.  Therefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models. We compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner. Our experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional vulnerabilities or performance penalty to the original model.", "histories": [], "reviews": [{"ORIGINALITY": 3, "CLARITY": 3, "is_meta_review": false, "comments": "I reviewed the manuscript as of December 6th. The authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The autho", "IS_META_REVIEW": false}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 4, "comments": "This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc.. It provides interesting observations", "ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false}, {"IMPACT": 2, "ORIGINALITY": 3, "is_meta_review": false, "comments": "The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et ", "IS_META_REVIEW": false}], "authors": "Xinyun Chen, Bo Li, Yevgeniy Vorobeychik", "accepted": false, "id": "610"}