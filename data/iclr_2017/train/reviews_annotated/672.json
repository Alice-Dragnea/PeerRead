{"conference": "ICLR 2017 conference submission", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 2, "comments": "The proposed method of modeling multimodal datasets is a VAE with an inference network for every combination of missing and present modalities. The method is evaluated on modeling MNIST and CelebA dat", "SOUNDNESS_CORRECTNESS": 2, "IS_META_REVIEW": false, "is_meta_review": false}, {"ORIGINALITY": 4, "SUBSTANCE": 3, "is_meta_review": false, "comments": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The", "IS_META_REVIEW": false}, {"IMPACT": 3, "ORIGINALITY": 3, "is_meta_review": false, "comments": "The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standa", "IS_META_REVIEW": false}], "authors": "Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo", "accepted": false, "id": "672"}