{"conference": "ICLR 2017 conference submission", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector\u2019s dimensionality on the resulting representations.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sent", "RECOMMENDATION": 5}, {"IMPACT": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine", "RECOMMENDATION": 5}, {"IMPACT": 4, "SUBSTANCE": 4, "comments": "This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and som", "SOUNDNESS_CORRECTNESS": 4, "is_meta_review": false, "RECOMMENDATION": 4, "IS_META_REVIEW": false}], "authors": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg", "accepted": true, "id": "489"}