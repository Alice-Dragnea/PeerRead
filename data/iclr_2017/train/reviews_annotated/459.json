{"conference": "ICLR 2017 conference submission", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "histories": [], "reviews": [{"IMPACT": 3, "comments": "The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning. The framework is nice and appealing. However, ", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_META_REVIEW": false, "is_meta_review": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing. A", "SOUNDNESS_CORRECTNESS": 5}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sha", "SOUNDNESS_CORRECTNESS": 5}], "authors": "Yongxin Yang, Timothy M. Hospedales", "accepted": true, "id": "459"}