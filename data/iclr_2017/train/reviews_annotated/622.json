{"conference": "ICLR 2017 conference submission", "title": "The loss surface of residual networks: Ensembles and the role of batch normalization", "abstract": "Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensemble are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network\u2019s depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.", "histories": [], "reviews": [{"ORIGINALITY": 2, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting c", "SOUNDNESS_CORRECTNESS": 4}, {"CLARITY": 3, "is_meta_review": false, "comments": "Summary: In this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of tr", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "This paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analys", "IS_META_REVIEW": false}], "authors": "Etai Littwin, Lior Wolf", "accepted": false, "id": "622"}