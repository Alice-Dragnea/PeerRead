{"conference": "ICLR 2017 conference submission", "title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "histories": [], "reviews": [{"IMPACT": 3, "ORIGINALITY": 3, "is_meta_review": false, "comments": "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, w", "IS_META_REVIEW": false}, {"comments": "The paper proposes a convergence analysis of some two-layer NNs with ReLUs. It is not the first such analysis, but maybe it is novel on the assumptions used in the analysis, and the focus on ReLU nonl", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "IS_META_REVIEW": false, "CLARITY": 3, "is_meta_review": false}, {"ORIGINALITY": 4, "is_meta_review": false, "comments": "This work analyzes the continuous-time dynamics of gradient descent when training two-layer ReLU networks (one input, one output, thus only one layer of ReLU units). The work is interesting in the sen", "IS_META_REVIEW": false}], "authors": "Yuandong Tian", "accepted": false, "id": "522"}