{"conference": "ICLR 2017 conference submission", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.", "histories": [], "reviews": [{"is_meta_review": false, "comments": "This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution. Pros: - Impressively retains accuracy on popular models on ImageNet and Cifar10", "IS_META_REVIEW": false}, {"IMPACT": 4, "is_meta_review": false, "comments": "This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress ne", "IS_META_REVIEW": false}, {"SUBSTANCE": 4, "comments": "This paper proposes a very simple idea (prune low-weight filters from ConvNets) in order to reduce FLOPs and memory consumption. The proposed method is experimented on with VGG-16 and ResNets on CIFAR", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "is_meta_review": false, "RECOMMENDATION": 5, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "The idea of \"pruning where it matters\" is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too. Extra points for c", "SOUNDNESS_CORRECTNESS": 5}], "authors": "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf", "accepted": true, "id": "324"}