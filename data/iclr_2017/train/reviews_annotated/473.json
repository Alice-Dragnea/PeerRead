{"conference": "ICLR 2017 conference submission", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling", "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance betw", "SOUNDNESS_CORRECTNESS": 5}, {"ORIGINALITY": 1, "is_meta_review": false, "comments": "This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax. Experiments on PTB shows significant improvement. The ide", "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mas", "SOUNDNESS_CORRECTNESS": 4}], "authors": "Hakan Inan, Khashayar Khosravi, Richard Socher", "accepted": true, "id": "473"}