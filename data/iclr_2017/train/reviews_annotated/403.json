{"conference": "ICLR 2017 conference submission", "title": "Lie-Access Neural Turing Machines", "abstract": "External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.", "histories": [], "reviews": [{"ORIGINALITY": 4, "CLARITY": 4, "is_meta_review": false, "comments": "The paper proposes a new memory access scheme based on Lie group actions for NTMs. Pros: * Well written * Novel addressing scheme as an extension to NTM. * Seems to work slightly better than normal NT", "IS_META_REVIEW": false}, {"IMPACT": 3, "comments": "The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagat", "ORIGINALITY": 5, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"ORIGINALITY": 4, "CLARITY": 5, "is_meta_review": false, "comments": "The Neural Turing Machine and related external memory models have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In par", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "*** Paper Summary *** This paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then ", "IS_META_REVIEW": false}], "authors": "Greg Yang, Alexander Rush", "accepted": true, "id": "403"}