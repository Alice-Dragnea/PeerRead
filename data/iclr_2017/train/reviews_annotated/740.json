{"conference": "ICLR 2017 conference submission", "title": "ParMAC: distributed optimisation of nested functions, with application to binary autoencoders", "abstract": "Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such \"nested\" functions is the \"method of auxiliary coordinates (MAC)\". MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. We describe ParMAC, a distributed-computation model for MAC. This trains on a dataset distributed across machines while limiting the amount of communication so it does not obliterate the benefit of parallelism. ParMAC works on a cluster of machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and its parallel speedup, and implement ParMAC using MPI to learn binary autoencoders for fast image retrieval, achieving nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points.", "histories": [], "reviews": [{"SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 4, "comments": "UPDATE: I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful. However, I am reviewing the submission and my overall assessment", "SOUNDNESS_CORRECTNESS": 3, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "SUBSTANCE": 3, "is_meta_review": false, "comments": "The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the", "RECOMMENDATION": 2}, {"SUBSTANCE": 3, "is_meta_review": false, "comments": "This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition o", "IS_META_REVIEW": false}], "authors": "Miguel A. Carreira-Perpinan, Mehdi Alizadeh", "accepted": false, "id": "740"}