{"conference": "ICLR 2017 conference submission", "title": "Unsupervised Perceptual Rewards for Imitation Learning", "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.", "histories": [], "reviews": [{"MEANINGFUL_COMPARISON": 3, "comments": "The paper tries to present a first step towards solving the difficult problem of \"learning from limited number of demonstrations\". The paper tries to present 3 contributions towards this effort: 1. un", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false}, {"ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes a novel method to learn vision feature as intermediate rewards to guide the robot training in the real world. Since there are only a few sequences of human demonstrations, the pape", "SOUNDNESS_CORRECTNESS": 4}, {"CLARITY": 5, "is_meta_review": false, "comments": "The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To o", "IS_META_REVIEW": false}], "authors": "Pierre Sermanet, Kelvin Xu, Sergey Levine", "accepted": false, "id": "504"}