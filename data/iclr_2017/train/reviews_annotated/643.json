{"conference": "ICLR 2017 conference submission", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "abstract": "In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model\u2019s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the \u201cneuronal birth\u201d is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. \u201cNeuronal death\u201d is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.", "histories": [], "reviews": [{"IMPACT": 3, "comments": "The paper is interesting, it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate (or even delete) codes as new data is comin", "ORIGINALITY": 3, "IS_META_REVIEW": false, "RECOMMENDATION": 3, "is_meta_review": false}, {"MEANINGFUL_COMPARISON": 3, "comments": "I'd like to thank the authors for their detailed response and clarifications. This work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of t", "ORIGINALITY": 2, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}, {"comments": "The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionar", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}], "authors": "Sahil Garg, Irina Rish, Guillermo Cecchi, Aurelie Lozano", "accepted": false, "id": "643"}