{"conference": "ICLR 2017 conference submission", "title": "Recurrent Batch Normalization", "abstract": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps.  We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experi", "SOUNDNESS_CORRECTNESS": 2}, {"ORIGINALITY": 2, "CLARITY": 5, "is_meta_review": false, "comments": "Contributions The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work de", "IS_META_REVIEW": false}, {"IMPACT": 4, "is_meta_review": false, "comments": "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that ", "IS_META_REVIEW": false}], "authors": "Tim Cooijmans, Nicolas Ballas, C\u00e9sar Laurent, \u00c7a\u011flar G\u00fcl\u00e7ehre, Aaron Courville", "accepted": true, "id": "434"}