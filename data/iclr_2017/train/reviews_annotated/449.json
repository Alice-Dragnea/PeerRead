{"conference": "ICLR 2017 conference submission", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Us", "SOUNDNESS_CORRECTNESS": 3}, {"IMPACT": 3, "is_meta_review": false, "comments": "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. This paper is", "IS_META_REVIEW": false}, {"comments": "This paper presents a strategy for building deep neural networks via rules for expansion and merging of sub-networks. pros: - the idea is novel - the approach is described clearly cons: - the experime", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 5, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}], "authors": "Gustav Larsson, Michael Maire, Gregory Shakhnarovich", "accepted": true, "id": "449"}