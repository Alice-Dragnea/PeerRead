{"conference": "ICLR 2017 conference submission", "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond", "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.", "histories": [], "reviews": [{"IMPACT": 4, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 2, "comments": "This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which me", "ORIGINALITY": 3, "is_meta_review": false, "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "comments": "Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of ", "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}, {"is_meta_review": false, "comments": "The paper analyzes the properties of the Hessian of the training objective for various neural networks and data distributions. The authors study in particular, the eigenspectrum of the Hessian, which ", "IS_META_REVIEW": false}], "authors": "Levent Sagun, Leon Bottou, Yann LeCun", "accepted": false, "id": "623"}