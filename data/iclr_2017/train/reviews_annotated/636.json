{"conference": "ICLR 2017 conference submission", "title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "SOUNDNESS_CORRECTNESS": 3, "is_meta_review": false, "comments": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observation", "MEANINGFUL_COMPARISON": 2}, {"IMPACT": 3, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper presents an approach for compensating the input/activation variance introduced by dropout in a network. Additionally, a practical inference trick of re-estimating the batch normalization par", "SOUNDNESS_CORRECTNESS": 5}, {"IMPACT": 5, "SUBSTANCE": 3, "comments": "The main observation made in the paper is that the use of dropout increases the variance of neurons. Correcting for this increase in variance, in the parameter initialization, and in the test-time sta", "ORIGINALITY": 2, "is_meta_review": false, "IS_META_REVIEW": false}], "authors": "Dan Hendrycks, Kevin Gimpel", "accepted": false, "id": "636"}