{"conference": "ICLR 2017 conference submission", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,    where we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{", "histories": [], "reviews": [{"ORIGINALITY": 4, "is_meta_review": false, "comments": "This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized meth", "IS_META_REVIEW": false}, {"SUBSTANCE": 4, "comments": "This an interesting investigation into learning rate schedules, bringing in the idea of restarts, often overlooked in deep learning. The paper does a thorough study on non-trivial datasets, and while ", "ORIGINALITY": 3, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "is_meta_review": false}, {"SUBSTANCE": 4, "is_meta_review": false, "comments": "This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task", "IS_META_REVIEW": false}], "authors": "Ilya Loshchilov, Frank Hutter", "accepted": true, "id": "435"}