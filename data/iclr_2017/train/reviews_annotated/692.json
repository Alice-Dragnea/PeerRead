{"conference": "ICLR 2017 conference submission", "title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "histories": [], "reviews": [{"IMPACT": 3, "MEANINGFUL_COMPARISON": 2, "comments": "The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature ", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "is_meta_review": false, "RECOMMENDATION": 4, "IS_META_REVIEW": false}, {"MEANINGFUL_COMPARISON": 2, "comments": "The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here: Please do not make incredibly unscientific statements like this one: \"The working procedure of this", "SOUNDNESS_CORRECTNESS": 1, "IS_META_REVIEW": false, "RECOMMENDATION": 1, "is_meta_review": false}, {"IMPACT": 3, "MEANINGFUL_COMPARISON": 2, "comments": "This paper presents a hierarchical attention-based method for document classification. The main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-b", "SOUNDNESS_CORRECTNESS": 3, "is_meta_review": false, "RECOMMENDATION": 2, "IS_META_REVIEW": false}], "authors": "Zhigang Yuan, Yuting Hu, Yongfeng Huang", "accepted": false, "id": "692"}