{"conference": "ICLR 2017 conference submission", "title": "Beyond Fine Tuning: A Modular Approach to Learning on Small Data", "abstract": "In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of  domain-specific hand-engineered features. Here we take the  approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.", "histories": [], "reviews": [{"IMPACT": 4, "comments": "This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard fine-tuning approach. P", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"IMPACT": 4, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 2, "comments": "This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea i", "ORIGINALITY": 4, "is_meta_review": false, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "SOUNDNESS_CORRECTNESS": 3, "is_meta_review": false, "comments": "This paper proposed to perform finetuning in an augmentation fashion by freezing the original network and adding a new model aside it. The idea itself is interesting and complements existing training ", "RECOMMENDATION": 4}], "authors": "Aryk Anderson, Kyle Shaffer, Artem Yankov, Court Corley, Nathan Hodas", "accepted": false, "id": "701"}