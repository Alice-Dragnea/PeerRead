{"conference": "ICLR 2017 conference submission", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.", "histories": [], "reviews": [{"IMPACT": 5, "SUBSTANCE": 4, "is_meta_review": false, "comments": "I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get adva", "IS_META_REVIEW": false}, {"APPROPRIATENESS": 5, "is_meta_review": false, "comments": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well", "IS_META_REVIEW": false}, {"IMPACT": 4, "comments": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. Pros and Cons: Although there is lit", "ORIGINALITY": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "is_meta_review": false}], "authors": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang", "accepted": true, "id": "315"}