{"conference": "ICLR 2017 conference submission", "title": "Investigating Different Context Types and Representations for Learning Word Embeddings", "abstract": "The number of word embedding models is growing every year. Most of them learn word embeddings based on the co-occurrence information of words and their context. However, it's still an open question what is the best definition of context. We provide the first systematical investigation of different context types and representations for learning word embeddings. We conduct comprehensive experiments to evaluate their effectiveness under 4 tasks (21 datasets), which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 2, "comments": "This paper analyzes dependency trees vs standard window contexts for word vector learning. While that's a good goal I believe the paper falls short of a thorough analysis of the subject matter. It doe", "ORIGINALITY": 2, "is_meta_review": false, "IS_META_REVIEW": false}, {"comments": "This paper investigates the issue of whether and how to use syntactic dependencies in unsupervised word representation learning models like CBOW or Skip-Gram, with a focus one the issue of bound (word", "SOUNDNESS_CORRECTNESS": 5, "IS_META_REVIEW": false, "RECOMMENDATION": 2, "APPROPRIATENESS": 2, "is_meta_review": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper evaluates how different context types affect the quality of word embeddings on a plethora of benchmarks. I am ambivalent about this paper. On one hand, it continues an important line of wor", "RECOMMENDATION": 3}], "authors": "Bofang Li, Tao Liu, Zhe Zhao, Buzhou Tang, Xiaoyong Du", "accepted": false, "id": "649"}