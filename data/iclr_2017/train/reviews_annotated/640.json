{"conference": "ICLR 2017 conference submission", "title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "histories": [], "reviews": [{"SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 2, "comments": "This paper discusses multi-sense embedddings and proposes learning those by using aligned text across languages. Further, the paper suggests that adding more languages helps improve word sense disambi", "ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false}, {"SUBSTANCE": 4, "CLARITY": 3, "is_meta_review": false, "comments": "this work aims to address representation of multi-sense words by exploiting multilingual context. Experiments on word sense induction and word similarity in context show that the proposed solution imp", "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "comments": "In this paper, the authors propose a Bayesian variant of the skipgram model to learn word embeddings. There are two important variant compared to the original model. First, aligned sentences from mult", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}], "authors": "Shyam Upadhyay, Kai-Wei Chang, James Zou, Matt Taddy, Adam Kalai", "accepted": false, "id": "640"}