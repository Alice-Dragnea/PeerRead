{"conference": "ICLR 2017 conference submission", "title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "histories": [], "reviews": [{"IMPACT": 5, "MEANINGFUL_COMPARISON": 3, "comments": "This paper proposes to leverage \"surprisal\" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a L", "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}, {"MEANINGFUL_COMPARISON": 1, "comments": "This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. -This", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 2, "IS_META_REVIEW": false, "is_meta_review": false}, {"SUBSTANCE": 3, "comments": "Summary: This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Autho", "ORIGINALITY": 3, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}], "authors": "Kamil Rocki", "accepted": false, "id": "793"}