{"conference": "ICLR 2017 conference submission", "title": "Semantic Noise Modeling for Better Representation Learning", "abstract": "Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption that good base representation for supervised tasks can be attained by maximizing the sum of hierarchical mutual informations between the input, latent, and output variables. From this base model, we introduce a semantic noise modeling method which enables semantic perturbation on the latent space to enhance the representational power of learned latent feature. During training, latent vector representation can be stochastically perturbed by a modeled additive noise while preserving its original semantics. It implicitly brings the effect of semantic augmentation on the latent space. The proposed model can be easily learned by back-propagation with common gradient-based optimization algorithms. Experimental results show that the proposed method helps to achieve performance benefits against various previous approaches. We also provide the empirical analyses for the proposed latent space modeling method including t-SNE visualization.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "SUBSTANCE": 2, "is_meta_review": false, "comments": "This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. Technical issues: The move from (1) to (2) is problematic. Yes it is a lower bound, but by", "SOUNDNESS_CORRECTNESS": 3}, {"SUBSTANCE": 4, "APPROPRIATENESS": 4, "comments": "The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is achieved by defining a measure", "is_meta_review": false, "RECOMMENDATION": 3, "CLARITY": 3, "IS_META_REVIEW": false}, {"SUBSTANCE": 2, "MEANINGFUL_COMPARISON": 4, "comments": "The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning.", "ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false}], "authors": "Hyo-Eun Kim, Sangheum Hwang, Kyunghyun Cho", "accepted": false, "id": "777"}