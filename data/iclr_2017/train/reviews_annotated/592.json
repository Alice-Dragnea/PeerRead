{"conference": "ICLR 2017 conference submission", "title": "Epitomic Variational Autoencoders", "abstract": "In this paper, we propose epitomic variational autoencoder (eVAE), a probabilistic generative model of high dimensional data. eVAE is composed of a number of sparse variational autoencoders called `epitome' such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. We show that the proposed model greatly overcomes the common problem in variational autoencoders (VAE) of model over-pruning. We substantiate that eVAE is efficient in using its model capacity and generalizes better than VAE, by presenting qualitative and quantitative results on MNIST and TFD datasets.", "histories": [], "reviews": [{"ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper replaces the Gaussian prior often used in a VAE with a group sparse prior. They modify the approximate posterior function so that it also generates group sparse samples. The development of ", "SOUNDNESS_CORRECTNESS": 3}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper presents a version of a variational autoencoder that uses a discrete latent variable that masks the activation of the latent code, making only a subset (an \"epitome\") of the latent variables", "RECOMMENDATION": 1}, {"IMPACT": 3, "comments": "This paper proposes an elegant solution to a very important problem in VAEs, namely that the model over-regularizes itself by killing off latent dimensions. People have used annealing of the KL term a", "ORIGINALITY": 4, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "is_meta_review": false}], "authors": "Serena Yeung, Anitha Kannan, Yann Dauphin, Li Fei-Fei", "accepted": false, "id": "592"}