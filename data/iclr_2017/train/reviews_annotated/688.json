{"conference": "ICLR 2017 conference submission", "title": "Efficient iterative policy optimization", "abstract": "We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates.", "histories": [], "reviews": [{"SUBSTANCE": 3, "is_meta_review": false, "comments": "This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family. I'm not familiar enough with this type lower bound scheme to comment o", "IS_META_REVIEW": false}, {"ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper presents an interesting modification to PoWER algorithm that is well motivated. The main limitation of this paper is the lack of comparison with other methods and on richer problems. The exp", "MEANINGFUL_COMPARISON": 3}, {"MEANINGFUL_COMPARISON": 4, "comments": "The paper considers the problem of reinforcement learning where the number of policy updates is required to be low. The problem is well motivated and the author provides an interesting modification to", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 5, "is_meta_review": false, "APPROPRIATENESS": 3, "IS_META_REVIEW": false}], "authors": "Nicolas Le Roux", "accepted": false, "id": "688"}