{"conference": "ICLR 2017 conference submission", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models, in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.", "histories": [], "reviews": [{"comments": "In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the i", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}, {"ORIGINALITY": 4, "CLARITY": 5, "is_meta_review": false, "comments": "I have not much to add to my pre-review comments. It's a very well written paper with an interesting idea. Lots of people currently want to combine RL with NLP. It is very en vogue. Nobody has gotten ", "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework ", "MEANINGFUL_COMPARISON": 4}], "authors": "Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling", "accepted": true, "id": "464"}