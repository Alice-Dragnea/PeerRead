{"conference": "ICLR 2017 conference submission", "title": "Inefficiency of stochastic gradient descent with larger mini-batches (and more learners)", "abstract": "Stochastic Gradient Descent (SGD) and its variants are the most important optimization algorithms used in large scale machine learning. Mini-batch version of stochastic gradient is often used in practice for taking advantage of hardware parallelism. In this work, we analyze the effect of mini-batch size over SGD convergence for the case of general non-convex objective functions. Building on the past analyses, we justify mathematically that there can often be a large difference between the convergence guarantees provided by small and large mini-batches (given each instance processes equal number of training samples), while providing experimental evidence for the same. Going further to distributed settings, we show that an analogous effect holds with popular Asynchronous Gradient Descent (\\asgd): there can be a large difference between convergence guarantees with increasing number of learners given that the cumulative number of training samples processed remains the same. Thus there is an inherent (and similar) inefficiency introduced in the convergence behavior when we attempt to take advantage of parallelism, either by increasing mini-batch size or by increase the number of learners.", "histories": [], "reviews": [{"IMPACT": 3, "comments": "This paper shows that when a larger mini-batch is used (in the serial setting), the number of samples needed to be processed for the same convergence guarantee is larger. A similar behavior is discuss", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false}, {"IMPACT": 4, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 3, "comments": "This paper addresses the problem of the influence of mini-batch size on the SGD convergence in a general non-convex setting. The results are then translated to analyze the influence of the number of l", "SOUNDNESS_CORRECTNESS": 3, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"is_meta_review": false, "comments": "This paper theoretically justified a faster convergence (in terms of average gradient norm attained after processing a fixed number of samples) of using small mini-batches for SGD or ASGD with smaller", "IS_META_REVIEW": false}], "authors": "Onkar Bhardwaj, Guojing Cong", "accepted": false, "id": "653"}