{"conference": "ICLR 2017 conference submission", "title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper uses a combination of likelihood and reward based learning to learn sequence models for music. The ability to combine likelihood and reward based learning has been long known, as a result o", "SOUNDNESS_CORRECTNESS": 5}, {"IMPACT": 3, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "The authors propose a solution for the task of synthesizing melodies. The authors claim that the \"language-model\"-type approaches with LSTMs generate melodies with certain shortcomings. They tend to l", "SOUNDNESS_CORRECTNESS": 5}, {"ORIGINALITY": 2, "is_meta_review": false, "comments": "This paper suggests combining LSTMs, trained on a large midi corpus, with a handcrafted reward function that helps to fine-tune the model in a musically meaningful way. The idea to use hand-crafted re", "IS_META_REVIEW": false}], "authors": "Natasha Jaques, Shixiang Gu, Richard E. Turner, Douglas Eck", "accepted": false, "id": "529"}