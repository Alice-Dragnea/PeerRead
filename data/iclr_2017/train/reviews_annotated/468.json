{"conference": "ICLR 2017 conference submission", "title": "Towards the Limit of Network Quantization", "abstract": "Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.", "histories": [], "reviews": [{"ORIGINALITY": 3, "is_meta_review": false, "comments": "This paper proposes a novel neural network compression technique. The goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss. It ", "IS_META_REVIEW": false}, {"SUBSTANCE": 4, "comments": "The paper has two main contributions: 1) Shows that uniform quantization works well with variable length (Huffman) coding 2) Improves fixed-length quantization by proposing the Hessian-weighted k-mean", "ORIGINALITY": 3, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"APPROPRIATENESS": 5, "comments": "This paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters. The authors assume that the", "ORIGINALITY": 2, "is_meta_review": false, "RECOMMENDATION": 5, "CLARITY": 5, "IS_META_REVIEW": false}], "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "accepted": true, "id": "468"}