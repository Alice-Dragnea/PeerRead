{"conference": "ICLR 2017 conference submission", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "SUBSTANCE": 3, "is_meta_review": false, "comments": "The proposed regularizer seems to be a particular combination of existing methods. Though the implied connection between nonlinearities and stochastic regularizers is intriguing, in my opinion the emp", "SOUNDNESS_CORRECTNESS": 3}, {"SUBSTANCE": 3, "comments": "The method proposed essential trains neural networks without a traditional nonlinearity, using multiplicative gating by the CDF of a Gaussian evaluated at the preactivation; this is motivated as a rel", "ORIGINALITY": 3, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "SUBSTANCE": 3, "is_meta_review": false, "comments": "Approaches like adaptive dropout also have the binary mask as a function of input to a neuron very similar to the proposed approach. It is not clear, even from the new draft, how the proposed approach", "MEANINGFUL_COMPARISON": 2}], "authors": "Dan Hendrycks, Kevin Gimpel", "accepted": false, "id": "651"}