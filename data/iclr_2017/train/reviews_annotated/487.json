{"conference": "ICLR 2017 conference submission", "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "histories": [], "reviews": [{"CLARITY": 3, "is_meta_review": false, "comments": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks", "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "is_meta_review": false, "comments": "From my original comments: The results looks good but the baselines proposed are quite bad. For instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with", "IS_META_REVIEW": false}, {"SUBSTANCE": 4, "MEANINGFUL_COMPARISON": 4, "comments": "Experimental results look reasonable, validated on 3 tasks. References could be improved, for example I would rather see Rumelhart's paper cited for back-propagation than the Deep Learning book.", "SOUNDNESS_CORRECTNESS": 5, "IS_META_REVIEW": false, "is_meta_review": false}], "authors": "Arash Ardakani, Carlo Condo, Warren J. Gross", "accepted": true, "id": "487"}