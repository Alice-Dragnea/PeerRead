{"conference": "ICLR 2017 conference submission", "title": "Tighter bounds lead to improved classifiers", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "histories": [], "reviews": [{"ORIGINALITY": 3, "SUBSTANCE": 2, "is_meta_review": false, "comments": "The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error ", "IS_META_REVIEW": false}, {"ORIGINALITY": 5, "CLARITY": 4, "is_meta_review": false, "comments": "The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are", "IS_META_REVIEW": false}, {"APPROPRIATENESS": 5, "comments": "The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution dev", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "CLARITY": 5, "is_meta_review": false}], "authors": "Nicolas Le Roux", "accepted": true, "id": "493"}