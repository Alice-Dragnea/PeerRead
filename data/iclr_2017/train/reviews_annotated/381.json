{"conference": "ICLR 2017 conference submission", "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference", "abstract": "We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation-a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "is_meta_review": false, "comments": "Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintai", "MEANINGFUL_COMPARISON": 3}, {"IS_META_REVIEW": false, "MEANINGFUL_COMPARISON": 3, "is_meta_review": false, "comments": "Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function w", "RECOMMENDATION": 2}, {"ORIGINALITY": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of ", "SOUNDNESS_CORRECTNESS": 5}], "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz", "accepted": true, "id": "381"}