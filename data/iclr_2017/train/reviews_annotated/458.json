{"conference": "ICLR 2017 conference submission", "title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "histories": [], "reviews": [{"CLARITY": 4, "is_meta_review": false, "comments": "The paper introduces a new regularization term which encourages the optimizer to search for a flat local minimum of reasonably low loss instead of seeking a sharp region of a low loss. This is motivat", "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "Overview: This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a compu", "SOUNDNESS_CORRECTNESS": 4}, {"ORIGINALITY": 4, "is_meta_review": false, "comments": "This paper presents a principled approach to finding flat minima. The motivation to seek such minima is due to their better generalization ability. The idea is to add to the original loss function a n", "IS_META_REVIEW": false}], "authors": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, Riccardo Zecchina", "accepted": true, "id": "458"}