{"conference": "ICLR 2017 conference submission", "title": "Efficient Communications in Training Large Scale Neural Networks", "abstract": "We consider the problem of how to reduce the cost of communication that is re- quired for the parallel training of a neural network. The state-of-the-art method, Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD), requires a many collective communication operations, like broadcasts of parameters or reduc- tions for sub-gradient aggregations, which for large messages quickly dominates overall execution time and limits parallel scalability. To address this problem, we develop a new technique for collective operations, referred to as Linear Pipelining (LP). It is tuned to the message sizes that arise in BSP-SGD, and works effectively on multi-GPU systems. Theoretically, the cost of LP is invariant to P , where P is the number of GPUs, while the cost of more conventional Minimum Spanning Tree (MST) scales like O(log P ). LP also demonstrate up to 2x faster bandwidth than Bidirectional Exchange (BE) techniques that are widely adopted by current MPI implementations. We apply these collectives to BSP-SGD, showing that the proposed implementations reduce communication bottlenecks in practice while preserving the attractive convergence properties of BSP-SGD.", "histories": [], "reviews": [{"SUBSTANCE": 4, "CLARITY": 3, "is_meta_review": false, "comments": "This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in ", "IS_META_REVIEW": false}, {"IMPACT": 3, "comments": "This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. Comments 1) The name linear pipeline is somewhat confusing to the readers, as the technique is u", "SOUNDNESS_CORRECTNESS": 5, "IS_META_REVIEW": false, "CLARITY": 4, "is_meta_review": false}, {"is_meta_review": false, "comments": "The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two indep", "IS_META_REVIEW": false}], "authors": "Linnan Wang, Wei Wu, George Bosilca, Richard Vuduc, Zenglin Xu", "accepted": false, "id": "780"}