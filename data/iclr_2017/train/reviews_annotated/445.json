{"conference": "ICLR 2017 conference submission", "title": "Dialogue Learning With Human-in-the-Loop", "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes.  Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime.  Finally, real experiments with Mechanical Turk validate the approach.", "histories": [], "reviews": [{"comments": "SUMMARY: This paper describes a set of experiments evaluating techniques for training a dialogue agent via reinforcement learning. A standard memory network architecture is trained on both bAbI and a ", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 1, "IS_META_REVIEW": false, "APPROPRIATENESS": 1, "is_meta_review": false}, {"comments": "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related", "ORIGINALITY": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 4, "CLARITY": 5, "is_meta_review": false}, {"MEANINGFUL_COMPARISON": 3, "comments": "As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a s", "ORIGINALITY": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "is_meta_review": false}], "authors": "Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, Jason Weston", "accepted": true, "id": "445"}