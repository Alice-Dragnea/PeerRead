{"conference": "ICLR 2017 conference submission", "title": "Dynamic Partition Models", "abstract": "We present a new approach for learning compact and intuitive distributed representations with binary encoding. Rather than summing up expert votes as in products of experts, we employ for each variable the opinion of the most reliable expert. Data points are hence explained through a partitioning of the variables into expert supports. The partitions are dynamically adapted based on which experts are active. During the learning phase we adopt a smoothed version of this model that uses separate mixtures for each data dimension. In our experiments we achieve accurate reconstructions of high-dimensional data points with at most a dozen experts.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "CLARITY": 1, "is_meta_review": false, "comments": "The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I ca", "RECOMMENDATION": 2}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "comments": "This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE. I find the paper very unclear. I tried t", "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 1, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "The goal of this paper is to learn a collection of experts that are individually meaningful and that have disjoint responsibilities. Unlike a standard mixture model, they use a different mixture for e", "MEANINGFUL_COMPARISON": 1}], "authors": "Marc Goessling, Yali Amit", "accepted": false, "id": "781"}