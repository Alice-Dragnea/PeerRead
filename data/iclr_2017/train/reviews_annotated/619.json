{"conference": "ICLR 2017 conference submission", "title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "histories": [], "reviews": [{"IMPACT": 2, "MEANINGFUL_COMPARISON": 1, "comments": "This paper presents a simple method of adding gradient noise to improve the training of deep neural networks. This paper first appeared on arXiv over a year ago and while there have been many innovati", "ORIGINALITY": 2, "is_meta_review": false, "RECOMMENDATION": 1, "APPROPRIATENESS": 1, "IS_META_REVIEW": false}, {"IMPACT": 3, "SUBSTANCE": 4, "MEANINGFUL_COMPARISON": 2, "comments": "The authors consider a simple optimization technique consisting of adding gradient noise with a specific schedule. They test their method on a number of recently proposed neural networks for simulatin", "ORIGINALITY": 3, "is_meta_review": false, "CLARITY": 4, "IS_META_REVIEW": false}, {"comments": "The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the me", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}], "authors": "Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Lukasz Kaiser, Karol Kurach, Ilya Sutskever, James Martens", "accepted": false, "id": "619"}