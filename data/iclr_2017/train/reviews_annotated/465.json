{"conference": "ICLR 2017 conference submission", "title": "Delving into Transferable Adversarial Examples and Black-box Attacks", "abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "CLARITY": 4, "is_meta_review": false, "comments": "I reviewed the manuscript as of December 7th. Summary: The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in lar", "SOUNDNESS_CORRECTNESS": 4}, {"ORIGINALITY": 4, "CLARITY": 3, "is_meta_review": false, "comments": "The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. Im on the fence about this paper but am leaning towards acceptance. Such detailed e", "IS_META_REVIEW": false}, {"IMPACT": 4, "comments": "This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of \"attacks\" in the context of image classication. Specifically, an attack aims to fool the class", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false}], "authors": "Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song", "accepted": true, "id": "465"}