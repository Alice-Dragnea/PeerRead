{"conference": "ICLR 2017 conference submission", "title": "Opening the vocabulary of  neural language models with character-level word representations", "abstract": "This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 3, "MEANINGFUL_COMPARISON": 1, "comments": "this paper proposes a model for representing unseen words in a neural language model. the proposed model achieves poor results in LM and a slight improvement over a baseline model. this work needs a m", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 2, "is_meta_review": false, "IS_META_REVIEW": false}, {"IMPACT": 3, "comments": "This paper proposes an extension of neural network language (NLM) models to better handle large vocabularies. The main idea is to obtain word embeddings by combining character-level embeddings with a ", "SOUNDNESS_CORRECTNESS": 3, "IS_META_REVIEW": false, "CLARITY": 3, "is_meta_review": false}, {"ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the", "MEANINGFUL_COMPARISON": 4}], "authors": "Matthieu Labeau, Alexandre Allauzen", "accepted": false, "id": "685"}