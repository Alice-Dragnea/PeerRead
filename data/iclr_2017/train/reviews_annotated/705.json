{"conference": "ICLR 2017 conference submission", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "histories": [], "reviews": [{"IMPACT": 3, "comments": "This work proposes a joint classification of images and audio captions for the task of word like discovery of acoustic units that correlate to semantically visual objects. The general this is a very i", "ORIGINALITY": 3, "IS_META_REVIEW": false, "RECOMMENDATION": 2, "is_meta_review": false}, {"ORIGINALITY": 2, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "CONTRIBUTIONS This paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both ima", "MEANINGFUL_COMPARISON": 4}, {"comments": "This paper is a follow-up on the NIPS 2016 paper \"Unsupervised learning of spoken language with visual context\", and does exactly what that paper proposes in its future work section: \"to perform acous", "ORIGINALITY": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 3, "CLARITY": 5, "is_meta_review": false}], "authors": "David Harwath, James R. Glass", "accepted": false, "id": "705"}