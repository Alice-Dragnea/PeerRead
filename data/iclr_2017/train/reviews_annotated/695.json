{"conference": "ICLR 2017 conference submission", "title": "A Simple yet Effective Method to Prune Dense Layers of Neural Networks", "abstract": "Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks and datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.", "histories": [], "reviews": [{"IMPACT": 4, "SUBSTANCE": 3, "comments": "The paper proposes to prune a neural network by removing neurons whose operation is highly correlated with other neurons. The idea is nice and somewhat novel - most pruning methods concentrate on remo", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "is_meta_review": false, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper proposes and tests two ideas. (1) a method of pruning networks by identifying highly correlated neuron pairs, pruning one of the pair, and then modifying downstream weights to compensate fo", "RECOMMENDATION": 2}, {"ORIGINALITY": 5, "CLARITY": 5, "is_meta_review": false, "comments": "Summary: In this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network. They do this by identifying pairs of neurons produce the most correlated outputs, ", "IS_META_REVIEW": false}], "authors": "Mohammad Babaeizadeh, Paris Smaragdis, Roy H. Campbell", "accepted": false, "id": "695"}