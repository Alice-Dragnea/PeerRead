{"conference": "ICLR 2017 conference submission", "title": "Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning", "abstract": "In this paper, we present multiple approaches for improving the performance of gradient descent when utilizing mutiple compute resources. The proposed approaches span a solution space ranging from equivalence to running on a single compute device to delaying gradient updates a fixed number of times. We present a new approach, asynchronous layer-wise gradient descent that maximizes overlap of layer-wise backpropagation (computation) with gradient synchronization (communication). This approach provides maximal theoretical equivalence to the de facto gradient descent algorithm, requires limited asynchronicity across multiple iterations of gradient descent, theoretically improves overall speedup, while minimizing the additional space requirements for asynchronicity. We implement all of our proposed approaches using Caffe \u2013 a high performance Deep Learning library \u2013 and evaluate it on both an Intel Sandy Bridge cluster connected with InfiniBand as well as an NVIDIA DGX-1 connected with NVLink. The evaluations are performed on a set of well known workloads including AlexNet and GoogleNet on the ImageNet dataset. Our evaluation of these neural network topologies indicates asynchronous gradient descent has a speedup of up to 1.7x compared to synchronous.", "histories": [], "reviews": [{"IMPACT": 1, "SUBSTANCE": 1, "MEANINGFUL_COMPARISON": 2, "comments": "The authors present methods to speed-up gradient descent by leveraging asynchronicity in a layer-wise manner. While they obtain up-to 1.7x speedup compared to synchronous training, their baseline is w", "SOUNDNESS_CORRECTNESS": 2, "is_meta_review": false, "RECOMMENDATION": 1, "IS_META_REVIEW": false}, {"IMPACT": 2, "MEANINGFUL_COMPARISON": 2, "comments": "This paper is relatively difficult to parse. Much of the exposition of the proposed algorithm could be better presented using pseudo-code describing the compute flow, or a diagram describing exactly h", "SOUNDNESS_CORRECTNESS": 2, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}, {"ORIGINALITY": 2, "is_meta_review": false, "comments": "This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training. Comments 1) The described manual implementation of delayed synchronization and state protection ", "IS_META_REVIEW": false}], "authors": "Jeff Daily, Abhinav Vishnu, Charles Siegel", "accepted": false, "id": "617"}