{"conference": "ICLR 2017 conference submission", "title": "Sigma Delta Quantized Networks", "abstract": "Deep neural networks can be obscenely wasteful. When processing video, a convolutional network expends a fixed amount of computation for each frame with no regard to the similarity between neighbouring frames. As a result, it ends up repeatedly doing very similar computations. To put an end to such waste, we introduce Sigma-Delta networks. With each new input, each layer in this network sends a discretized form of its change in activation to the next layer. Thus the amount of computation that the network does scales with the amount of change in the input and layer activations, rather than the size of the network. We introduce an optimization method for converting any pre-trained deep network into an optimally efficient Sigma-Delta network, and show that our algorithm, if run on the appropriate hardware, could cut at least an order of magnitude from the computational cost of processing video data.", "histories": [], "reviews": [{"is_meta_review": false, "comments": "The paper presents a method to improve the efficiency of CNNs that encode sequential inputs in a slow fashion such that there is only a small change between the representation of adjacent steps in the", "IS_META_REVIEW": false}, {"IMPACT": 3, "comments": "This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent i", "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"ORIGINALITY": 3, "CLARITY": 4, "is_meta_review": false, "comments": "This is an interesting paper about quantized networks that work on temporal difference inputs. The basic idea is that when a network has only to process differences then this is computational much mor", "IS_META_REVIEW": false}], "authors": "Peter O'Connor, Max Welling", "accepted": true, "id": "413"}