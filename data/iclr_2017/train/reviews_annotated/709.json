{"conference": "ICLR 2017 conference submission", "title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then  fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models.  Our main result is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English->German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English->German. On summarization, our method beats the supervised learning baseline.", "histories": [], "reviews": [{"is_meta_review": false, "comments": "strengths: A method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all", "IS_META_REVIEW": false}, {"ORIGINALITY": 1, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity ", "MEANINGFUL_COMPARISON": 3}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine ", "RECOMMENDATION": 5}], "authors": "Prajit Ramachandran, Peter J. Liu, Quoc V. Le", "accepted": false, "id": "709"}