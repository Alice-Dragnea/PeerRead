{"conference": "ICLR 2017 conference submission", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.", "histories": [], "reviews": [{"CLARITY": 5, "is_meta_review": false, "comments": "The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units", "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "This paper shows: 1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units. 2. Extensions of the previous results to more general", "RECOMMENDATION": 4}, {"ORIGINALITY": 4, "is_meta_review": false, "comments": "SUMMARY This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is t", "IS_META_REVIEW": false}], "authors": "Shiyu Liang, R. Srikant", "accepted": true, "id": "495"}