{"conference": "ICLR 2017 conference submission", "title": "Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory", "abstract": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "histories": [], "reviews": [{"comments": "This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) ", "SOUNDNESS_CORRECTNESS": 5, "IS_META_REVIEW": false, "RECOMMENDATION": 2, "CLARITY": 4, "is_meta_review": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "[Summary] This paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned d", "IS_META_REVIEW": false}, {"IMPACT": 5, "ORIGINALITY": 5, "is_meta_review": false, "comments": "In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed app", "IS_META_REVIEW": false}], "authors": "Yelong Shen*, Po-Sen Huang*, Ming-Wei Chang, Jianfeng Gao", "accepted": false, "id": "596"}