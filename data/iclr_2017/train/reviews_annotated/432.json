{"conference": "ICLR 2017 conference submission", "title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.", "histories": [], "reviews": [{"CLARITY": 3, "is_meta_review": false, "comments": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-", "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG. Pr", "SOUNDNESS_CORRECTNESS": 5}, {"IMPACT": 5, "comments": "This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have ", "SOUNDNESS_CORRECTNESS": 5, "is_meta_review": false, "RECOMMENDATION": 5, "CLARITY": 5, "IS_META_REVIEW": false}], "authors": "Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, Volodymyr Mnih", "accepted": true, "id": "432"}