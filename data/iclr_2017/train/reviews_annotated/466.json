{"conference": "ICLR 2017 conference submission", "title": "Identity Matters in Deep Learning", "abstract": "An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.  In this work, we put the principle of identity parameterization on a more  solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for feed-forward networks in their standard parameterization is substantially more delicate.  Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size.  Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.", "histories": [], "reviews": [{"CLARITY": 4, "is_meta_review": false, "comments": "Paper Summary: Authors investigate identity re-parametrization in the linear and the non linear case. Detailed comments: Linear Residual Network: The paper shows that for a linear residual network any", "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. Re", "SOUNDNESS_CORRECTNESS": 5}, {"CLARITY": 5, "is_meta_review": false, "comments": "This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks w", "IS_META_REVIEW": false}], "authors": "Moritz Hardt, Tengyu Ma", "accepted": true, "id": "466"}