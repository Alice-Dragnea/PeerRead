{"conference": "ICLR 2017 conference submission", "title": "Nonparametric Neural Networks", "abstract": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce {\\it nonparametric neural networks}, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an $\\ell_p$ penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an $\\ell_2$ penalty. We employ a novel optimization algorithm, which we term ``Adaptive Radial-Angular Gradient Descent'' or {\\it AdaRad}, and obtain promising results.", "histories": [], "reviews": [{"IMPACT": 4, "comments": "This paper addresses the problem of allowing networks to change the number of units that are used during training. This is done in a simple but elegant and well-motivated way. Units with zero input or", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 5, "is_meta_review": false, "CLARITY": 5, "IS_META_REVIEW": false}, {"ORIGINALITY": 4, "SUBSTANCE": 4, "is_meta_review": false, "comments": "This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regula", "IS_META_REVIEW": false}, {"ORIGINALITY": 5, "CLARITY": 4, "is_meta_review": false, "comments": "I agree with reviewer 2 on the interesting part of the paper. The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines require", "IS_META_REVIEW": false}], "authors": "George Philipp, Jaime G. Carbonell", "accepted": true, "id": "322"}