{"conference": "ICLR 2017 conference submission", "title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "histories": [], "reviews": [{"IMPACT": 3, "SUBSTANCE": 3, "comments": "The paper describes an extension of the HasheNets work, with several novel twists. Instead of using a single hash function, the proposed HFH approach uses multiple hash function to associate each \"vir", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 3, "is_meta_review": false, "RECOMMENDATION": 2, "IS_META_REVIEW": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks. The authors show that this complex proposal is ", "SOUNDNESS_CORRECTNESS": 2}, {"IMPACT": 4, "comments": "The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where par", "ORIGINALITY": 5, "IS_META_REVIEW": false, "RECOMMENDATION": 5, "is_meta_review": false}], "authors": "Lei Shi, Shikun Feng, Zhifan Zhu", "accepted": false, "id": "686"}