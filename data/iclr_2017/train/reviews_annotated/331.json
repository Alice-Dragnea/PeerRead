{"conference": "ICLR 2017 conference submission", "title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning", "abstract": "People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where twp agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of ``analogy making,'' or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.", "histories": [], "reviews": [{"is_meta_review": false, "comments": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks. A core component of the proposed approac", "IS_META_REVIEW": false}, {"ORIGINALITY": 2, "is_meta_review": false, "comments": "This paper explores transfer in reinforcement learning between agents that may be morphologically distinct. The key idea is for the source and target agent to have learned a shared skill, and then to ", "IS_META_REVIEW": false}, {"SUBSTANCE": 4, "MEANINGFUL_COMPARISON": 4, "comments": "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "is_meta_review": false}], "authors": "Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, Sergey Levine", "accepted": true, "id": "331"}