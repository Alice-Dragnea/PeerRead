{"conference": "ICLR 2017 conference submission", "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "histories": [], "reviews": [{"IMPACT": 4, "comments": "In supervised learning, a significant advance occurred when the framework of semi-supervised learning was adopted, which used the weaker approach of unsupervised learning to infer some property, such ", "ORIGINALITY": 4, "IS_META_REVIEW": false, "RECOMMENDATION": 4, "is_meta_review": false}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "This paper formalizes the problem setting of having only a subset of available MDPs for which one has access to a reward. The authors name this setting \"semi-supervised reinforcement learning\" (SSRL),", "SOUNDNESS_CORRECTNESS": 5}, {"ORIGINALITY": 3, "CLARITY": 5, "is_meta_review": false, "comments": "The paper proposes to study the problem of semi-supervised RL where one has to distinguish between labelled MDPs that provide rewards, and unlabelled MDPs that are not associated with any reward signa", "IS_META_REVIEW": false}], "authors": "Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, Sergey Levine", "accepted": true, "id": "360"}