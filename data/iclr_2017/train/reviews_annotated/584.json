{"conference": "ICLR 2017 conference submission", "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "histories": [], "reviews": [{"IMPACT": 5, "SUBSTANCE": 3, "comments": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic rel", "SOUNDNESS_CORRECTNESS": 3, "IS_META_REVIEW": false, "is_meta_review": false}, {"ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as pipeline the later tasks will depending on the output of the previous tasks. Here, the author", "SOUNDNESS_CORRECTNESS": 3}, {"is_meta_review": false, "comments": "The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semanti", "IS_META_REVIEW": false}], "authors": "Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher", "accepted": false, "id": "584"}