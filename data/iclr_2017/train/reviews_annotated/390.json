{"conference": "ICLR 2017 conference submission", "title": "Metacontrol for Adaptive Imagination-Based Optimization", "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.", "histories": [], "reviews": [{"IMPACT": 4, "comments": "This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "CLARITY": 5, "is_meta_review": false}, {"SUBSTANCE": 3, "is_meta_review": false, "comments": "Thank you for an interesting read on an approach to choose computational models based on kind of examples given. Pros - As an idea, using a meta controller to decide the computational model and the nu", "IS_META_REVIEW": false}, {"IMPACT": 3, "CLARITY": 4, "is_meta_review": false, "comments": "Pros (quality, clarity, originality, significance:): This paper presents a novel metacontroller optimization system that learns the best action for a one-shot learning task, but as a framework has the", "IS_META_REVIEW": false}, {"IMPACT": 4, "CLARITY": 3, "is_meta_review": false, "comments": "A well written paper and an interesting construction - I thoroughly enjoyed reading it. I found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first wh", "IS_META_REVIEW": false}], "authors": "Jessica B. Hamrick, Andrew J. Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas Heess, Peter W. Battaglia", "accepted": true, "id": "390"}