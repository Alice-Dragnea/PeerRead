{"conference": "ICLR 2017 conference submission", "title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure.  This point of view also exposes the relation between compression and the minimum description length (MDL) principle.", "histories": [], "reviews": [{"ORIGINALITY": 3, "is_meta_review": false, "comments": "This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors. A mixture model prior over the weights leads to a clustering effect in the weig", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compres", "IS_META_REVIEW": false}, {"ORIGINALITY": 3, "is_meta_review": false, "comments": "This paper revives a classic idea involving regularization for purposes of compression for modern CNN models on resource constrained devices. Model compression is hot and we're in the midst of lots of", "IS_META_REVIEW": false}], "authors": "Karen Ullrich, Edward Meeds, Max Welling", "accepted": true, "id": "345"}