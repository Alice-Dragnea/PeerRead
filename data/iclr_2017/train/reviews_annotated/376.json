{"conference": "ICLR 2017 conference submission", "title": "Capacity and Trainability in Recurrent Neural Networks", "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.", "histories": [], "reviews": [{"comments": "CONTRIBUTIONS Large-scale experiments are used to measure the capacity and trainability of different RNN architectures. Capacity experiments suggest that across all architectures, RNNs can store betwe", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 5, "is_meta_review": false, "RECOMMENDATION": 5, "CLARITY": 5, "IS_META_REVIEW": false}, {"comments": "This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the ba", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 1, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}, {"IMPACT": 4, "comments": "The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and acti", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 3, "is_meta_review": false, "RECOMMENDATION": 5, "IS_META_REVIEW": false}], "authors": "Jasmine Collins, Jascha Sohl-Dickstein, David Sussillo", "accepted": true, "id": "376"}