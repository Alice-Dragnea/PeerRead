{"conference": "ICLR 2017 conference submission", "title": "Charged Point Normalization: An Efficient Solution to the Saddle Point Problem", "abstract": "Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks.", "histories": [], "reviews": [{"IMPACT": 5, "SUBSTANCE": 3, "comments": "This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This met", "ORIGINALITY": 4, "is_meta_review": false, "IS_META_REVIEW": false}, {"IMPACT": 3, "SUBSTANCE": 3, "comments": "The research direction taken by this paper is of great interest. However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). \"Throughout thi", "ORIGINALITY": 4, "is_meta_review": false, "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "comments": "Summary: This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a posi", "SOUNDNESS_CORRECTNESS": 3, "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 3, "IS_META_REVIEW": false}], "authors": "Armen Aghajanyan", "accepted": false, "id": "548"}