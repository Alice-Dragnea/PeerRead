{"conference": "ICLR 2017 conference submission", "title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "histories": [], "reviews": [{"CLARITY": 4, "is_meta_review": false, "comments": "Summary: In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. They define a cl", "IS_META_REVIEW": false}, {"ORIGINALITY": 2, "is_meta_review": false, "comments": "The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call ", "IS_META_REVIEW": false}, {"CLARITY": 5, "is_meta_review": false, "comments": "SUMMARY This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. PROS Interes", "IS_META_REVIEW": false}], "authors": "Ronen Basri, David W. Jacobs", "accepted": true, "id": "485"}