{"conference": "ICLR 2017 conference submission", "title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks", "abstract": "Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization.  The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions.", "histories": [], "reviews": [{"IMPACT": 4, "SUBSTANCE": 2, "comments": "This paper develops a theoretical guarantee for the convergence of the training error. The result is quite general that covers the training of a wide range of neural network models. The key idea of th", "SOUNDNESS_CORRECTNESS": 2, "ORIGINALITY": 4, "is_meta_review": false, "RECOMMENDATION": 2, "CLARITY": 3, "IS_META_REVIEW": false}, {"is_meta_review": false, "comments": "It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, ", "IS_META_REVIEW": false}, {"IMPACT": 4, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper adopts Taylor approximations of neural nets for separating convex and non-convex components of the optimization. This enables them to bound the training error by the Taylor optimum and regr", "SOUNDNESS_CORRECTNESS": 5}], "authors": "David Balduzzi, Brian McWilliams, Tony Butler-Yeoman", "accepted": false, "id": "526"}