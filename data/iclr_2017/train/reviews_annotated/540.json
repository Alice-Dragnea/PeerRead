{"conference": "ICLR 2017 conference submission", "title": "Modularized Morphing of Neural Networks", "abstract": "In this work we study the problem of network morphism, an effective learning scheme to morph a well-trained neural network to a new one with the network function completely preserved. Different from existing work where basic morphing types on the layer level were addressed, we target at the central problem of network morphism at a higher level, i.e., how a convolutional layer can be morphed into an arbitrary module of a neural network. To simplify the representation of a network, we abstract a module as a graph with blobs as vertices and convolutional layers as edges, based on which the morphing process is able to be formulated as a graph transformation problem. Two atomic morphing operations are introduced to compose the graphs, based on which modules are classified into two families, i.e., simple morphable modules and complex modules. We present practical morphing solutions for both of these two families, and prove that any reasonable module can be morphed from a single convolutional layer. Extensive experiments have been conducted based on the state-of-the-art ResNet on benchmark datasets, and the effectiveness of the proposed solution has been verified.", "histories": [], "reviews": [{"SUBSTANCE": 3, "comments": "The paper presents an interesting incremental approach for exploring new convolutional network hierarchies in an incremental manner after a baseline network has reached a good recognition performance.", "ORIGINALITY": 3, "is_meta_review": false, "CLARITY": 4, "IS_META_REVIEW": false}, {"comments": "This paper studies knowledge transfer problem from small capacity network to bigger one. This is a follow-up work of Net2Net (ICLR 2015) and NetMorph(ICML 2016). Comments - 1) This paper studies macro", "SOUNDNESS_CORRECTNESS": 4, "ORIGINALITY": 5, "IS_META_REVIEW": false, "APPROPRIATENESS": 4, "is_meta_review": false}, {"IS_META_REVIEW": false, "is_meta_review": false, "comments": "This paper presents works on neural network / CNN architecture morphing. Results are not reported on ImageNet larger ResNet and new network architecture such as Xception and DenseNet - which are maybe", "SOUNDNESS_CORRECTNESS": 3}, {"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "The paper proposes a methodology for morphing a trained network to different architecture without having to retrain from scratch. The manuscript reads well and the description is easy to follow. Howev", "MEANINGFUL_COMPARISON": 1}], "authors": "Tao Wei, Changhu Wang, Chang Wen Chen", "accepted": false, "id": "540"}