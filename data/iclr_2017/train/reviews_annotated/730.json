{"conference": "ICLR 2017 conference submission", "title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "histories": [], "reviews": [{"IMPACT": 4, "SUBSTANCE": 3, "comments": "This paper presents three improvements to the standard LSTM architecture used in many neural NLP models: Monte Carlo averaging, embed average pooling, and residual connections. Each of the modificatio", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "is_meta_review": false}, {"IMPACT": 3, "is_meta_review": false, "comments": "I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in ", "IS_META_REVIEW": false}, {"SUBSTANCE": 3, "CLARITY": 5, "is_meta_review": false, "comments": "The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhanc", "IS_META_REVIEW": false}], "authors": "Shayne Longpre, Sabeek Pradhan, Caiming Xiong, Richard Socher", "accepted": false, "id": "730"}