{"conference": "ICLR 2017 conference submission", "title": "Multi-modal Variational Encoder-Decoders", "abstract": "Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, uni-modal priors \u2014 such as the multivariate Gaussian distribution \u2014 yet many real-world data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable models are applied to these domains, the restriction of the simple, uni-modal prior hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution. To overcome this critical restriction, we propose a flexible, simple prior distribution which can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We develop the multi-modal variational encoder-decoder framework and investigate the effectiveness of the proposed prior in several natural language processing modeling tasks, including document modeling and dialogue modeling.", "histories": [], "reviews": [{"comments": "UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread. My thoughts have not changed. The authors propose using a mixture prior rather than a uni-modal prior for ", "SOUNDNESS_CORRECTNESS": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 1, "CLARITY": 2, "is_meta_review": false}, {"SUBSTANCE": 2, "comments": "This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. The", "ORIGINALITY": 4, "is_meta_review": false, "CLARITY": 2, "IS_META_REVIEW": false}, {"SUBSTANCE": 4, "comments": "The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing mul", "SOUNDNESS_CORRECTNESS": 4, "IS_META_REVIEW": false, "CLARITY": 3, "is_meta_review": false}], "authors": "Iulian V. Serban, Alexander G. Ororbia II, Joelle Pineau, Aaron Courville", "accepted": false, "id": "706"}