{"conference": "ICLR 2017 conference submission", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "histories": [], "reviews": [{"IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false, "comments": "SYNOPSIS: The authors introduce an efficient approximation to the softmax function that speeds up the empirical calculation of the softmax on GPUs. They leverage the unbalanced distribution of words a", "MEANINGFUL_COMPARISON": 4}, {"CLARITY": 5, "is_meta_review": false, "comments": "he authors provide an interesting, computational-complexity-driven approach for efficient softmax computation for language modeling based on GPUs. An adaptive softmax approach is proposed based on a h", "IS_META_REVIEW": false}, {"CLARITY": 3, "is_meta_review": false, "comments": "The authors introduce an adaptive softmax approximation tailored for faster performance on GPUs. The key idea, which is very sensible, is to use a class-based hierarchical softmax, but where the clust", "IS_META_REVIEW": false}], "authors": "\u00c9douard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, Herv\u00e9 J\u00e9gou", "accepted": false, "id": "523"}