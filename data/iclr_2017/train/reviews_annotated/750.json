{"conference": "ICLR 2017 conference submission", "title": "An Analysis of Feature Regularization for Low-shot Learning", "abstract": "Low-shot visual learning, the ability to recognize novel object categories from very few, or even one example, is a hallmark of human visual intelligence. Though successful on many tasks, deep learning approaches tends to be notoriously data-hungry. Recently, feature penalty regularization has been proved effective on capturing new concepts. In this work, we provide both empirical evidence and theoretical analysis on how and why these methods work. We also propose a better design of cost function with improved performance. Close scrutiny reveals the centering effect of feature representation, as well as the intrinsic connection with batch normalization. Extensive experiments on synthetic datasets, the one-shot learning benchmark \u201cOmniglot\u201d, and large-scale ImageNet validate our analysis.", "histories": [], "reviews": [{"comments": "The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. Although the equations suggest a weighting per example, dropping this weight (alpha_i) works", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 2, "IS_META_REVIEW": false, "RECOMMENDATION": 3, "is_meta_review": false}, {"IS_META_REVIEW": false, "SUBSTANCE": 3, "is_meta_review": false, "comments": "This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue tha", "SOUNDNESS_CORRECTNESS": 5}, {"ORIGINALITY": 3, "IS_META_REVIEW": false, "is_meta_review": false, "comments": "Summary === This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes and it was shown to ai", "SOUNDNESS_CORRECTNESS": 5}], "authors": "Zhuoyuan Chen, Han Zhao, Xiao Liu, Wei Xu", "accepted": false, "id": "750"}